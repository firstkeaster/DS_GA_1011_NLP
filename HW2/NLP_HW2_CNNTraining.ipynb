{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_HW2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "MzY7dBa-TgS5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gp1XLdI0TujS",
        "colab_type": "code",
        "outputId": "ddf018d1-9ef4-4286-8da7-85500c38bfa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x5bdb0000 @  0x7fa68b79e2a4 0x594e17 0x626104 0x51190a 0x4f5277 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x4f3338 0x510fb0 0x5119bd 0x4f6070\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V4D7O3jCTlTn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lsBItyr1UFW1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XJNNHUJvZI-b",
        "colab_type": "code",
        "outputId": "098546e6-eb02-41dc-e3b9-4e108214f97c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package libfuse2:amd64.\n",
            "(Reading database ... 22280 files and directories currently installed.)\n",
            "Preparing to unpack .../libfuse2_2.9.7-1ubuntu1_amd64.deb ...\n",
            "Unpacking libfuse2:amd64 (2.9.7-1ubuntu1) ...\n",
            "Selecting previously unselected package fuse.\n",
            "Preparing to unpack .../fuse_2.9.7-1ubuntu1_amd64.deb ...\n",
            "Unpacking fuse (2.9.7-1ubuntu1) ...\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.0-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.0-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up libfuse2:amd64 (2.9.7-1ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Setting up fuse (2.9.7-1ubuntu1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.0-0ubuntu1~ubuntu18.04.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Qt3SDFOYYY0h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import io\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from collections import Counter\n",
        "import pickle as pkl\n",
        "import string\n",
        "import random\n",
        "import pdb\n",
        "import nltk\n",
        "from nltk import ngrams\n",
        "from collections import Counter\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rmJhS0RzenVX",
        "colab_type": "code",
        "outputId": "6116d400-79fc-4ccf-90ac-a44bcb6320dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla K80'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "mVhugHTjz5TA",
        "colab_type": "code",
        "outputId": "f4b1ca2b-de6c-4e94-9910-98dbc943a83b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "ac = torch.Tensor([3,4,5])\n",
        "ac.cuda()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 3.,  4.,  5.], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "Tt7uNpFoUgIR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "random.seed(152)\n",
        "\n",
        "PAD_IDX = 0\n",
        "UNK_IDX = 1\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "max_vocab_size = 10000\n",
        "# save index 0 for unk and 1 for pad\n",
        "PAD_IDX = 0\n",
        "UNK_IDX = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "87YCImnjUkFk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nowpath = '/content/drive/DL Me/NLP_HW2'\n",
        "datapath = nowpath + '/hw2_data'\n",
        "picklePath = nowpath + '/hw2_data_p'\n",
        "resPath = nowpath + '/hw2_data_cheatreal'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VEPbuBiPuLiH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open(resPath + '/waww.p', 'w+') as ff:\n",
        "    ff.write('fuck')\n",
        "    ff.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vfrWlQZ8UlhL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_train = []\n",
        "all_val = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "67nvGpY1Um_B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def snli_reader(dataset, filepath):\n",
        "    with open(filepath) as f:\n",
        "        f.readline()\n",
        "        lines = f.readlines()\n",
        "        for line in lines:\n",
        "            dataset.append([x.strip('. \\n') for x in line.split('\\t')])\n",
        "        f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-FTb_QzvVwKC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "snli_reader(all_train, datapath + '/snli_train.tsv')\n",
        "snli_reader(all_val, datapath + '/snli_val.tsv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KiuuGPgaWwUJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_4d2LpJhXV7R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pkl_dumper(objct, file_name):\n",
        "    with open(file_name, 'wb') as f:\n",
        "        pkl.dump(objct, f, protocol=None)\n",
        "    return\n",
        "\n",
        "def pkl_loader(file_name):\n",
        "    with open(file_name, 'rb') as f:\n",
        "        objct = pkl.load(f)\n",
        "    return(objct)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oJdsOlUqX5S2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_embedded_dict = pkl_loader(picklePath + '/train_embedded_dict_pad_samelen_try2.p')\n",
        "val_embedded_dict = pkl_loader(picklePath + '/val_embedded_dict_pad_samelen_try2.p')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f5ZrwAjUZcXI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class VocabDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
        "    Note that this class inherits torch.utils.data.Dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cen_1, cen_2, tgt, method='concat'):\n",
        "        \"\"\"\n",
        "        @param data_list: list of character\n",
        "        @param target_list: list of targets\n",
        "\n",
        "        \"\"\"\n",
        "        self.cen_1_list, self.cen_2_list, self.target_list = cen_1, cen_2, tgt\n",
        "        self.method = method\n",
        "        self.interact = {\n",
        "            'concat':lambda x, y: x + y\n",
        "            \n",
        "        }\n",
        "        assert (len(self.cen_1_list) == len(self.target_list) == len(self.cen_2_list))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.cen_1_list)\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        \"\"\"\n",
        "        Triggered when you call dataset[i]\n",
        "        \"\"\"\n",
        "        \n",
        "        #tot_cen = self.interact[method](self.cen_1_list[key], self.cen_2_list[key])\n",
        "        cen_1, cen_2 = self.cen_1_list[key], self.cen_2_list[key]\n",
        "        label = self.target_list[key]\n",
        "        return [cen_1, cen_2, len(cen_1) + len(cen_2), label]\n",
        "\n",
        "def vocab_collate_func(batch):\n",
        "    \"\"\"\n",
        "    Customized function for DataLoader that dynamically pads the batch so that all\n",
        "    data have the same length\n",
        "    \"\"\"\n",
        "    data_list_1 = []\n",
        "    data_list_2 = []\n",
        "    label_list = []\n",
        "    length_list = []\n",
        "    \n",
        "    data_list_1, data_list_2, length_list, label_list = zip(*batch)\n",
        "\n",
        "    # for datum in batch:\n",
        "    #     label_list.append(datum[2])\n",
        "    #     length_list.append(datum[1])\n",
        "    #     data_list.append(datum[0])\n",
        "    return [torch.from_numpy(np.array(data_list_1)), torch.from_numpy(np.array(data_list_2)), torch.LongTensor(length_list), torch.LongTensor(label_list)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ccx04WjHd-HY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_dataset = VocabDataset(train_embedded_dict['org'], train_embedded_dict['alt'], train_embedded_dict['label'])\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=vocab_collate_func,\n",
        "                                           shuffle=True)\n",
        "\n",
        "val_dataset = VocabDataset(val_embedded_dict['org'], val_embedded_dict['alt'], val_embedded_dict['label'])\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=vocab_collate_func,\n",
        "                                           shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GMxU2E7geCOS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class GRU(nn.Module):\n",
        "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, bidirectional=False):\n",
        "        # RNN Accepts the following hyperparams:\n",
        "        # emb_size: Embedding Size\n",
        "        # hidden_size: Hidden Size of layer in RNN\n",
        "        # num_layers: number of layers in RNN\n",
        "        # num_classes: number of output classes\n",
        "        # vocab_size: vocabulary size\n",
        "        super(GRU, self).__init__()\n",
        "\n",
        "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
        "        self.bidirectional = bidirectional\n",
        "        #self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
        "        self.rnn_c1 = nn.GRU(emb_size, hidden_size//2, num_layers, batch_first=True, bidirectional=bidirectional)\n",
        "        #self.rnn_c2 = nn.RNN(emb_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear_1 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear_2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # Function initializes the activation of recurrent neural net at timestep 0\n",
        "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
        "        if self.bidirectional == True:\n",
        "            hidden = torch.randn(self.num_layers*2, batch_size, self.hidden_size//2)\n",
        "        else:\n",
        "            hidden = torch.randn(self.num_layers, batch_size, self.hidden_size)\n",
        "        return hidden.cuda()\n",
        "\n",
        "    def forward(self, c1, c2, lengths):\n",
        "        # reset hidden state\n",
        "        #print(c1.size())\n",
        "        batch_size, seq_len, emb_size = c1.size()\n",
        "\n",
        "        self.hidden = self.init_hidden(batch_size)\n",
        "        #print(self.hidden_1.size())\n",
        "        # get embedding of characters\n",
        "        c1_rnn_out, c1_hidden = self.rnn_c1(c1, self.hidden)\n",
        "        c2_rnn_out, c2_hidden = self.rnn_c1(c2, self.hidden)\n",
        "        #print('embed_size:',embed.size())\n",
        "        # pack padded sequence\n",
        "        \n",
        "        # fprop though RNN\n",
        "        \n",
        "        # print('rnn_out_size:',rnn_out.size())\n",
        "        # print('hidden_size:',self.hidden.size())\n",
        "        # undo packing\n",
        "        concated_hidden = torch.cat((c1_hidden, c2_hidden), 2)\n",
        "        concated_hidden, _ = torch.max(concated_hidden, dim=0)\n",
        "        #print(concated_hidden.size())\n",
        "        l1_out = self.linear_1(concated_hidden)\n",
        "        l2_out = self.linear_2(l1_out)\n",
        "        #print(l2_out.size())\n",
        "        #print('rnn_out_size_2:',rnn_out.size())\n",
        "        # sum hidden activations of RNN across time\n",
        "        #rnn_out = torch.sum(rnn_out, dim=1)\n",
        "        #print('rnn_out_size_final:',rnn_out.size())\n",
        "        #logits = self.linear(rnn_out)\n",
        "        #print('logits_size_final:',logits.size())\n",
        "        return l2_out.cuda()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hGJ8YkLLeXDH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_model(loader, model):\n",
        "    \"\"\"\n",
        "    Help function that tests the model's performance on a dataset\n",
        "    @param: loader - data loader for the dataset to test against\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    for i, (cen_1, cen_2, lengths, labels) in enumerate(loader):\n",
        "        cen_1_batch, cen_2_batch, lengths_batch, label_batch = cen_1.float().cuda(), cen_2.float().cuda(), lengths.cuda(), labels.cuda()\n",
        "        outputs = F.softmax(model(cen_1_batch, cen_2_batch, lengths_batch), dim=1).cuda()\n",
        "        predicted = outputs.max(1, keepdim=True)[1]\n",
        "        predicted = predicted.cuda()\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels.view_as(predicted).cuda()).sum().item()\n",
        "        # if i == 200:\n",
        "        #     break\n",
        "    return (100 * correct / total)\n",
        "\n",
        "\n",
        "model = GRU(emb_size=300, hidden_size=200, num_layers=1, num_classes=3, bidirectional = True)\n",
        "\n",
        "learning_rate = 2e-3\n",
        "num_epochs = 3 # number epoch to train\n",
        "\n",
        "# Criterion and Optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "model.cuda()\n",
        "criterion.cuda()\n",
        "\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "\n",
        "train_loss_list = []\n",
        "val_acc_list = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (cen_1, cen_2, lengths, labels) in enumerate(train_loader):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        outputs = model(cen_1.float().cuda(), cen_2.float().cuda(), lengths.cuda())\n",
        "        #print(outputs.size())\n",
        "        #print(labels.size())\n",
        "        loss = criterion(outputs, labels.cuda())\n",
        "\n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # if i == 200:\n",
        "        #     break\n",
        "        # validate every 100 iterations\n",
        "        if i % 100 == 0:\n",
        "          print(i)\n",
        "        if i > 0 and i % 1000 == 0:\n",
        "            # validate\n",
        "            val_acc = test_model(val_loader, model)\n",
        "            val_acc_list.append(val_acc)\n",
        "            train_loss = 100 - test_model(train_loader, model)\n",
        "            train_loss_list.append(train_loss)\n",
        "            print('Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}, Validation Acc: {}'.format(\n",
        "                       epoch+1, num_epochs, i+1, len(train_loader), train_loss, val_acc))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IWyc70LzeiRt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!nvidia_smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NkOA4X6kemb9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pkl_dumper(train_loss_list, picklePath + '/train_loss_list_200h_concat.p')\n",
        "pkl_dumper(val_acc_list, picklePath + '/val_acc_list_200h_concat.p')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s29DaqtDvvL4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pkl_dumper(model, picklePath + '/model_200h_concat.p')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "To8zYm6Hv1sm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_model(loader, model):\n",
        "    \"\"\"\n",
        "    Help function that tests the model's performance on a dataset\n",
        "    @param: loader - data loader for the dataset to test against\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    for i, (cen_1, cen_2, lengths, labels) in enumerate(loader):\n",
        "        cen_1_batch, cen_2_batch, lengths_batch, label_batch = cen_1.float().cuda(), cen_2.float().cuda(), lengths.cuda(), labels.cuda()\n",
        "        outputs = F.softmax(model(cen_1_batch, cen_2_batch, lengths_batch), dim=1).cuda()\n",
        "        predicted = outputs.max(1, keepdim=True)[1]\n",
        "        predicted = predicted.cuda()\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels.view_as(predicted).cuda()).sum().item()\n",
        "        # if i == 200:\n",
        "        #     break\n",
        "    return (100 * correct / total)\n",
        "\n",
        "\n",
        "model = GRU(emb_size=300, hidden_size=400, num_layers=1, num_classes=3, bidirectional = True)\n",
        "\n",
        "learning_rate = 2e-3\n",
        "num_epochs = 3 # number epoch to train\n",
        "\n",
        "# Criterion and Optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "model.cuda()\n",
        "criterion.cuda()\n",
        "print('num_parameters:', sum(p.numel() for p in model.parameters()))\n",
        "\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "\n",
        "train_loss_list_2 = []\n",
        "val_acc_list_2 = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (cen_1, cen_2, lengths, labels) in enumerate(train_loader):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        outputs = model(cen_1.float().cuda(), cen_2.float().cuda(), lengths.cuda())\n",
        "        #print(outputs.size())\n",
        "        #print(labels.size())\n",
        "        loss = criterion(outputs, labels.cuda())\n",
        "\n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # if i == 200:\n",
        "        #     break\n",
        "        # validate every 100 iterations\n",
        "        if i % 100 == 0:\n",
        "          print(i)\n",
        "        if i > 0 and i % 1000 == 0:\n",
        "            # validate\n",
        "            val_acc = test_model(val_loader, model)\n",
        "            val_acc_list_2.append(val_acc)\n",
        "            train_loss = 100 - test_model(train_loader, model)\n",
        "            train_loss_list_2.append(train_loss)\n",
        "            print('Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}, Validation Acc: {}'.format(\n",
        "                       epoch+1, num_epochs, i+1, len(train_loader), train_loss, val_acc))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bC-yeQW3GbLa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pkl_dumper(train_loss_list_2, picklePath + '/train_loss_list_400h_concat.p')\n",
        "pkl_dumper(val_acc_list_2, picklePath + '/val_acc_list_400h_concat.p')\n",
        "pkl_dumper(model, picklePath + '/model_400h_concat.p')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pCownyGBe8Cv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "del(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zHKKWdPMfTJW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_model(loader, model):\n",
        "    \"\"\"\n",
        "    Help function that tests the model's performance on a dataset\n",
        "    @param: loader - data loader for the dataset to test against\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    for i, (cen_1, cen_2, lengths, labels) in enumerate(loader):\n",
        "        cen_1_batch, cen_2_batch, lengths_batch, label_batch = cen_1.float().cuda(), cen_2.float().cuda(), lengths.cuda(), labels.cuda()\n",
        "        outputs = F.softmax(model(cen_1_batch, cen_2_batch, lengths_batch), dim=1).cuda()\n",
        "        predicted = outputs.max(1, keepdim=True)[1]\n",
        "        predicted = predicted.cuda()\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels.view_as(predicted).cuda()).sum().item()\n",
        "        # if i == 200:\n",
        "        #     break\n",
        "    return (100 * correct / total)\n",
        "\n",
        "\n",
        "model = GRU(emb_size=300, hidden_size=50, num_layers=1, num_classes=3, bidirectional = True)\n",
        "\n",
        "learning_rate = 2e-3\n",
        "num_epochs = 3 # number epoch to train\n",
        "\n",
        "# Criterion and Optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "model.cuda()\n",
        "criterion.cuda()\n",
        "print('num_parameters:', sum(p.numel() for p in model.parameters()))\n",
        "\n",
        "\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "\n",
        "train_loss_list_3 = []\n",
        "val_acc_list_3 = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (cen_1, cen_2, lengths, labels) in enumerate(train_loader):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        outputs = model(cen_1.float().cuda(), cen_2.float().cuda(), lengths.cuda())\n",
        "        #print(outputs.size())\n",
        "        #print(labels.size())\n",
        "        loss = criterion(outputs, labels.cuda())\n",
        "\n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # if i == 200:\n",
        "        #     break\n",
        "        # validate every 100 iterations\n",
        "        if i % 100 == 0:\n",
        "          print(i)\n",
        "        if i > 0 and i % 1000 == 0:\n",
        "            # validate\n",
        "            val_acc = test_model(val_loader, model)\n",
        "            val_acc_list_3.append(val_acc)\n",
        "            train_loss = 100 - test_model(train_loader, model)\n",
        "            train_loss_list_3.append(train_loss)\n",
        "            print('Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}, Validation Acc: {}'.format(\n",
        "                       epoch+1, num_epochs, i+1, len(train_loader), train_loss, val_acc))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_1dErBaJfd7K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pkl_dumper(train_loss_list_3, picklePath + '/train_loss_list_50h_concat.p')\n",
        "pkl_dumper(val_acc_list_3, picklePath + '/val_acc_list_50h_concat.p')\n",
        "pkl_dumper(model, picklePath + '/model_50h_concat.p')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hWVVlOFlgFTk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "del(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DcW72uh5gsXY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Interacte the two sentences with element-wise multiplication\n",
        "\n",
        "class GRU(nn.Module):\n",
        "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, bidirectional=False):\n",
        "        # RNN Accepts the following hyperparams:\n",
        "        # emb_size: Embedding Size\n",
        "        # hidden_size: Hidden Size of layer in RNN\n",
        "        # num_layers: number of layers in RNN\n",
        "        # num_classes: number of output classes\n",
        "        # vocab_size: vocabulary size\n",
        "        super(GRU, self).__init__()\n",
        "\n",
        "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
        "        self.bidirectional = bidirectional\n",
        "        #self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
        "        self.rnn_c1 = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
        "        #self.rnn_c2 = nn.RNN(emb_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear_1 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear_2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # Function initializes the activation of recurrent neural net at timestep 0\n",
        "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
        "        if self.bidirectional == True:\n",
        "            hidden = torch.randn(self.num_layers*2, batch_size, self.hidden_size)\n",
        "        else:\n",
        "            hidden = torch.randn(self.num_layers, batch_size, self.hidden_siz)\n",
        "        return hidden.cuda()\n",
        "\n",
        "    def forward(self, c1, c2, lengths):\n",
        "        # reset hidden state\n",
        "        #print(c1.size())\n",
        "        batch_size, seq_len, emb_size = c1.size()\n",
        "\n",
        "        self.hidden = self.init_hidden(batch_size)\n",
        "        #print(self.hidden_1.size())\n",
        "        # get embedding of characters\n",
        "        c1_rnn_out, c1_hidden = self.rnn_c1(c1, self.hidden)\n",
        "        c2_rnn_out, c2_hidden = self.rnn_c1(c2, self.hidden)\n",
        "        #print('embed_size:',embed.size())\n",
        "        # pack padded sequence\n",
        "        \n",
        "        # fprop though RNN\n",
        "        \n",
        "        # print('rnn_out_size:',rnn_out.size())\n",
        "        # print('hidden_size:',self.hidden.size())\n",
        "        # undo packing\n",
        "        multed_hidden = c1_hidden * c2_hidden\n",
        "        multed_hidden, _ = torch.max(multed_hidden, dim=0)\n",
        "        \n",
        "        # concated_hidden = torch.cat((c1_hidden, c2_hidden), 2)\n",
        "        # concated_hidden, _ = torch.max(concated_hidden, dim=0)\n",
        "        #print(concated_hidden.size())\n",
        "        l1_out = self.linear_1(multed_hidden)\n",
        "        l2_out = self.linear_2(l1_out)\n",
        "        #print(l2_out.size())\n",
        "        #print('rnn_out_size_2:',rnn_out.size())\n",
        "        # sum hidden activations of RNN across time\n",
        "        #rnn_out = torch.sum(rnn_out, dim=1)\n",
        "        #print('rnn_out_size_final:',rnn_out.size())\n",
        "        #logits = self.linear(rnn_out)\n",
        "        #print('logits_size_final:',logits.size())\n",
        "        return l2_out.cuda()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hWjV4xwRZKqg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_model(loader, model):\n",
        "    \"\"\"\n",
        "    Help function that tests the model's performance on a dataset\n",
        "    @param: loader - data loader for the dataset to test against\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    for i, (cen_1, cen_2, lengths, labels) in enumerate(loader):\n",
        "        cen_1_batch, cen_2_batch, lengths_batch, label_batch = cen_1.float().cuda(), cen_2.float().cuda(), lengths.cuda(), labels.cuda()\n",
        "        outputs = F.softmax(model(cen_1_batch, cen_2_batch, lengths_batch), dim=1).cuda()\n",
        "        predicted = outputs.max(1, keepdim=True)[1]\n",
        "        predicted = predicted.cuda()\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels.view_as(predicted).cuda()).sum().item()\n",
        "        # if i == 200:\n",
        "        #     break\n",
        "    return (100 * correct / total)\n",
        "\n",
        "\n",
        "model = GRU(emb_size=300, hidden_size=100, num_layers=1, num_classes=3, bidirectional = True)\n",
        "\n",
        "learning_rate = 2e-3\n",
        "num_epochs = 3 # number epoch to train\n",
        "\n",
        "# Criterion and Optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "model.cuda()\n",
        "criterion.cuda()\n",
        "print('num_parameters:', sum(p.numel() for p in model.parameters()))\n",
        "\n",
        "\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "\n",
        "train_loss_list_4 = []\n",
        "val_acc_list_4 = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (cen_1, cen_2, lengths, labels) in enumerate(train_loader):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        outputs = model(cen_1.float().cuda(), cen_2.float().cuda(), lengths.cuda())\n",
        "        #print(outputs.size())\n",
        "        #print(labels.size())\n",
        "        loss = criterion(outputs, labels.cuda())\n",
        "\n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # if i == 200:\n",
        "        #     break\n",
        "        # validate every 100 iterations\n",
        "        if i % 100 == 0:\n",
        "          print(i)\n",
        "        if i > 0 and i % 1000 == 0:\n",
        "            # validate\n",
        "            val_acc = test_model(val_loader, model)\n",
        "            val_acc_list_4.append(val_acc)\n",
        "            train_loss = 100 - test_model(train_loader, model)\n",
        "            train_loss_list_4.append(train_loss)\n",
        "            print('Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}, Validation Acc: {}'.format(\n",
        "                       epoch+1, num_epochs, i+1, len(train_loader), train_loss, val_acc))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R2HE3lEMb1aw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pkl_dumper(train_loss_list_4, picklePath + '/train_loss_list_mult_100h_concat.p')\n",
        "pkl_dumper(val_acc_list_4, picklePath + '/val_acc_list_mult_100h_concat.p')\n",
        "pkl_dumper(model, picklePath + '/model_mult_100h_concat.p')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9MffIwDFczuY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "del(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uQtXtauGc1LW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Interacte the two sentences with element-wise subtraction, in representation of Euclidean distance of the two tensors.\n",
        "\n",
        "class GRU(nn.Module):\n",
        "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, bidirectional=False):\n",
        "        # RNN Accepts the following hyperparams:\n",
        "        # emb_size: Embedding Size\n",
        "        # hidden_size: Hidden Size of layer in RNN\n",
        "        # num_layers: number of layers in RNN\n",
        "        # num_classes: number of output classes\n",
        "        # vocab_size: vocabulary size\n",
        "        super(GRU, self).__init__()\n",
        "\n",
        "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
        "        self.bidirectional = bidirectional\n",
        "        #self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
        "        self.rnn_c1 = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
        "        #self.rnn_c2 = nn.RNN(emb_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear_1 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear_2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # Function initializes the activation of recurrent neural net at timestep 0\n",
        "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
        "        if self.bidirectional == True:\n",
        "            hidden = torch.randn(self.num_layers*2, batch_size, self.hidden_size)\n",
        "        else:\n",
        "            hidden = torch.randn(self.num_layers, batch_size, self.hidden_siz)\n",
        "        return hidden.cuda()\n",
        "\n",
        "    def forward(self, c1, c2, lengths):\n",
        "        # reset hidden state\n",
        "        #print(c1.size())\n",
        "        batch_size, seq_len, emb_size = c1.size()\n",
        "\n",
        "        self.hidden = self.init_hidden(batch_size)\n",
        "        #print(self.hidden_1.size())\n",
        "        # get embedding of characters\n",
        "        c1_rnn_out, c1_hidden = self.rnn_c1(c1, self.hidden)\n",
        "        c2_rnn_out, c2_hidden = self.rnn_c1(c2, self.hidden)\n",
        "        #print('embed_size:',embed.size())\n",
        "        # pack padded sequence\n",
        "        \n",
        "        # fprop though RNN\n",
        "        \n",
        "        # print('rnn_out_size:',rnn_out.size())\n",
        "        # print('hidden_size:',self.hidden.size())\n",
        "        # undo packing\n",
        "        multed_hidden = c1_hidden - c2_hidden\n",
        "        multed_hidden, _ = torch.max(multed_hidden, dim=0)\n",
        "        \n",
        "        # concated_hidden = torch.cat((c1_hidden, c2_hidden), 2)\n",
        "        # concated_hidden, _ = torch.max(concated_hidden, dim=0)\n",
        "        #print(concated_hidden.size())\n",
        "        l1_out = self.linear_1(multed_hidden)\n",
        "        l2_out = self.linear_2(l1_out)\n",
        "        #print(l2_out.size())\n",
        "        #print('rnn_out_size_2:',rnn_out.size())\n",
        "        # sum hidden activations of RNN across time\n",
        "        #rnn_out = torch.sum(rnn_out, dim=1)\n",
        "        #print('rnn_out_size_final:',rnn_out.size())\n",
        "        #logits = self.linear(rnn_out)\n",
        "        #print('logits_size_final:',logits.size())\n",
        "        return l2_out.cuda()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MMQXnTbNbgBJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_model(loader, model):\n",
        "    \"\"\"\n",
        "    Help function that tests the model's performance on a dataset\n",
        "    @param: loader - data loader for the dataset to test against\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    for i, (cen_1, cen_2, lengths, labels) in enumerate(loader):\n",
        "        cen_1_batch, cen_2_batch, lengths_batch, label_batch = cen_1.float().cuda(), cen_2.float().cuda(), lengths.cuda(), labels.cuda()\n",
        "        outputs = F.softmax(model(cen_1_batch, cen_2_batch, lengths_batch), dim=1).cuda()\n",
        "        predicted = outputs.max(1, keepdim=True)[1]\n",
        "        predicted = predicted.cuda()\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels.view_as(predicted).cuda()).sum().item()\n",
        "        # if i == 200:\n",
        "        #     break\n",
        "    return (100 * correct / total)\n",
        "\n",
        "\n",
        "model = GRU(emb_size=300, hidden_size=100, num_layers=1, num_classes=3, bidirectional = True)\n",
        "\n",
        "learning_rate = 2e-3\n",
        "num_epochs = 3 # number epoch to train\n",
        "\n",
        "# Criterion and Optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "model.cuda()\n",
        "criterion.cuda()\n",
        "print('num_parameters:', sum(p.numel() for p in model.parameters()))\n",
        "\n",
        "\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "\n",
        "train_loss_list_5 = []\n",
        "val_acc_list_5 = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (cen_1, cen_2, lengths, labels) in enumerate(train_loader):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        outputs = model(cen_1.float().cuda(), cen_2.float().cuda(), lengths.cuda())\n",
        "        #print(outputs.size())\n",
        "        #print(labels.size())\n",
        "        loss = criterion(outputs, labels.cuda())\n",
        "\n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # if i == 200:\n",
        "        #     break\n",
        "        # validate every 100 iterations\n",
        "        if i % 100 == 0:\n",
        "          print(i)\n",
        "        if i > 0 and i % 1000 == 0:\n",
        "            # validate\n",
        "            val_acc = test_model(val_loader, model)\n",
        "            val_acc_list_5.append(val_acc)\n",
        "            train_loss = 100 - test_model(train_loader, model)\n",
        "            train_loss_list_5.append(train_loss)\n",
        "            print('Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}, Validation Acc: {}'.format(\n",
        "                       epoch+1, num_epochs, i+1, len(train_loader), train_loss, val_acc))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_JJFmSMvbhM4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pkl_dumper(train_loss_list_5, picklePath + '/train_loss_list_subt_100h_concat.p')\n",
        "pkl_dumper(val_acc_list_5, picklePath + '/val_acc_list_subt_100h_concat.p')\n",
        "pkl_dumper(model, picklePath + '/model_subt_100h_concat.p')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tqTOzV09cf_9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "del(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SNyeqyUEcgpZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##Let's try CNN!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "33_eEdIsui-v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# CNN, with concatenated hidden state\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, kernel_size, padding_size, len_sen):\n",
        "        # RNN Accepts the following hyperparams:\n",
        "        # emb_size: Embedding Size\n",
        "        # hidden_size: Hidden Size of layer in RNN\n",
        "        # num_layers: number of layers in RNN\n",
        "        # num_classes: number of output classes\n",
        "        # vocab_size: vocabulary size\n",
        "        super(CNN, self).__init__()\n",
        "        #length of sentence, length of feature size\n",
        "        self.len_sen = len_sen\n",
        "        self.Lout_size = self.len_sen + 2*padding_size - 1*(kernel_size-1) \n",
        "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
        "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=kernel_size, padding=padding_size)\n",
        "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=kernel_size, padding=padding_size)\n",
        "        #self.rnn_c2 = nn.RNN(emb_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear_1 = nn.Linear(self.Lout_size * 2, self.Lout_size)\n",
        "        self.linear_2 = nn.Linear(self.Lout_size, num_classes)\n",
        "\n",
        "    def forward(self, c1, c2, lengths):\n",
        "        # reset hidden state\n",
        "        #print(c1.size())\n",
        "        batch_size, seq_len, emb_size = c1.size()\n",
        "\n",
        "        #print(self.hidden_1.size())\n",
        "        # get embedding of characters\n",
        "        c1_hidden = self.conv1(c1.transpose(1,2)).transpose(1,2)\n",
        "        c1_hidden = F.relu(c1_hidden.contiguous().view(-1, c1_hidden.size(-1))).view(batch_size, seq_len, c1_hidden.size(-1))\n",
        "        c1_hidden = self.conv2(c1_hidden.transpose(1,2)).transpose(1,2)\n",
        "        c1_hidden = F.relu(c1_hidden.contiguous().view(-1, c1_hidden.size(-1))).view(batch_size, seq_len, c1_hidden.size(-1))\n",
        "        \n",
        "        c2_hidden = self.conv1(c2.transpose(1,2)).transpose(1,2)\n",
        "        c2_hidden = F.relu(c2_hidden.contiguous().view(-1, c2_hidden.size(-1))).view(batch_size, seq_len, c2_hidden.size(-1))\n",
        "        c2_hidden = self.conv2(c2_hidden.transpose(1,2)).transpose(1,2)\n",
        "        c2_hidden = F.relu(c2_hidden.contiguous().view(-1, c2_hidden.size(-1))).view(batch_size, seq_len, c2_hidden.size(-1))\n",
        "        \n",
        "        #Concat, max pool\n",
        "        concated_hidden = torch.cat((c1_hidden, c2_hidden), 1)\n",
        "        concated_hidden, _ = torch.max(concated_hidden, dim=2)\n",
        "        #print('embed_size:',embed.size())\n",
        "        # pack padded sequence\n",
        "        \n",
        "        # fprop though RNN\n",
        "        \n",
        "        # print('rnn_out_size:',rnn_out.size())\n",
        "        # print('hidden_size:',self.hidden.size())\n",
        "        # undo packing\n",
        "        \n",
        "        l1_out = self.linear_1(concated_hidden)\n",
        "        l1_out = F.relu(l1_out.contiguous())\n",
        "        l2_out = self.linear_2(l1_out)\n",
        "        #print(l2_out.size())\n",
        "        #print('rnn_out_size_2:',rnn_out.size())\n",
        "        # sum hidden activations of RNN across time\n",
        "        #rnn_out = torch.sum(rnn_out, dim=1)\n",
        "        #print('rnn_out_size_final:',rnn_out.size())\n",
        "        #logits = self.linear(rnn_out)\n",
        "        #print('logits_size_final:',logits.size())\n",
        "        #print(l2_out.size())\n",
        "        return l2_out.cuda()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HvXIyCDRvw_K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1323
        },
        "outputId": "08b23337-d907-4ecd-f9ca-d0061de6e902"
      },
      "cell_type": "code",
      "source": [
        "def test_model(loader, model):\n",
        "    \"\"\"\n",
        "    Help function that tests the model's performance on a dataset\n",
        "    @param: loader - data loader for the dataset to test against\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    for i, (cen_1, cen_2, lengths, labels) in enumerate(loader):\n",
        "        cen_1_batch, cen_2_batch, lengths_batch, label_batch = cen_1.float().cuda(), cen_2.float().cuda(), lengths.cuda(), labels.cuda()\n",
        "        outputs = F.softmax(model(cen_1_batch, cen_2_batch, lengths_batch), dim=1).cuda()\n",
        "        predicted = outputs.max(1, keepdim=True)[1]\n",
        "        predicted = predicted.cuda()\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels.view_as(predicted).cuda()).sum().item()\n",
        "        # if i == 10:\n",
        "        #     break\n",
        "    return (100 * correct / total)\n",
        "\n",
        "\n",
        "model = CNN(emb_size=300, hidden_size=100, num_layers=2, num_classes=3, kernel_size=5, padding_size=2, len_sen=78)\n",
        "\n",
        "learning_rate = 2e-3\n",
        "num_epochs = 3 # number epoch to train\n",
        "\n",
        "# Criterion and Optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "model.cuda()\n",
        "criterion.cuda()\n",
        "print('num_parameters:', sum(p.numel() for p in model.parameters()))\n",
        "\n",
        "train_loss_list_cnn_1 = []\n",
        "val_acc_list_cnn_1 = []\n",
        "train_crossN_loss_list_cnn_1 = []\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (cen_1, cen_2, lengths, labels) in enumerate(train_loader):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        outputs = model(cen_1.float().cuda(), cen_2.float().cuda(), lengths.cuda())\n",
        "        #print(outputs.size())\n",
        "        #print(labels.size())\n",
        "        #print(labels)\n",
        "        loss = criterion(outputs, labels.cuda())\n",
        "        print(loss.item())\n",
        "\n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # if i == 20:\n",
        "        #     break\n",
        "        if i % 100 == 0:\n",
        "          print(i)\n",
        "        if i > 0 and i % 1000 == 0:\n",
        "            # validate\n",
        "            val_acc = test_model(val_loader, model)\n",
        "            val_acc_list_cnn_1.append(val_acc)\n",
        "            train_loss = 100 - test_model(train_loader, model)\n",
        "            train_crossN_loss = loss.item()\n",
        "            train_crossN_loss_list_cnn_1.append(train_crossN_loss)\n",
        "            train_loss_list_cnn_1.append(train_loss)\n",
        "            print('Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}, Validation Acc: {}'.format(\n",
        "                       epoch+1, num_epochs, i+1, len(train_loader), train_loss, val_acc))\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_parameters: 212683\n",
            "1.0968812704086304\n",
            "0\n",
            "1.1157417297363281\n",
            "1.069642424583435\n",
            "1.0687707662582397\n",
            "1.2088505029678345\n",
            "1.1488947868347168\n",
            "1.0938690900802612\n",
            "1.104777455329895\n",
            "1.0815101861953735\n",
            "1.1137640476226807\n",
            "1.1044647693634033\n",
            "1.0858345031738281\n",
            "1.0947140455245972\n",
            "1.08472740650177\n",
            "1.0651978254318237\n",
            "1.1305228471755981\n",
            "1.0619028806686401\n",
            "1.10977303981781\n",
            "1.132292628288269\n",
            "1.0838117599487305\n",
            "1.1107934713363647\n",
            "1.1307833194732666\n",
            "1.1071466207504272\n",
            "1.1412967443466187\n",
            "1.1126255989074707\n",
            "1.1040631532669067\n",
            "1.0877481698989868\n",
            "1.120862603187561\n",
            "1.0934362411499023\n",
            "1.0898950099945068\n",
            "1.113542914390564\n",
            "1.0784306526184082\n",
            "1.0932337045669556\n",
            "1.1120558977127075\n",
            "1.1149743795394897\n",
            "1.0448172092437744\n",
            "1.0854949951171875\n",
            "1.1130506992340088\n",
            "1.0978320837020874\n",
            "1.0989551544189453\n",
            "1.1076737642288208\n",
            "1.0632009506225586\n",
            "1.0650736093521118\n",
            "1.1371331214904785\n",
            "1.0885834693908691\n",
            "1.0713236331939697\n",
            "1.052142858505249\n",
            "1.096178412437439\n",
            "1.0884366035461426\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-511198b385df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcen_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcen_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-88d3825a6020>\u001b[0m in \u001b[0;36mvocab_collate_func\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m#     length_list.append(datum[1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m#     data_list.append(datum[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "HA9ewb3LwydS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# CNN, with concatenated hidden state\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, kernel_size, padding_size, len_sen, interact='concat'):\n",
        "        # RNN Accepts the following hyperparams:\n",
        "        # emb_size: Embedding Size\n",
        "        # hidden_size: Hidden Size of layer in RNN\n",
        "        # num_layers: number of layers in RNN\n",
        "        # num_classes: number of output classes\n",
        "        # vocab_size: vocabulary size\n",
        "        super(CNN, self).__init__()\n",
        "        #length of sentence, length of feature size\n",
        "        self.len_sen = len_sen\n",
        "        self.interact = interact\n",
        "        self.Lout_size = self.len_sen + 2*padding_size - 1*(kernel_size-1) \n",
        "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
        "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=kernel_size, padding=padding_size)\n",
        "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=kernel_size, padding=padding_size)\n",
        "        #self.rnn_c2 = nn.RNN(emb_size, hidden_size, num_layers, batch_first=True)\n",
        "        if self.interact == 'concat':\n",
        "            self.linear_1 = nn.Linear(self.Lout_size * 2, self.Lout_size)\n",
        "        else:\n",
        "            self.linear_1 = nn.Linear(self.Lout_size, self.Lout_size)\n",
        "        self.linear_2 = nn.Linear(self.Lout_size, num_classes)\n",
        "        self.interact_method = {\n",
        "            'concat': lambda x, y: torch.cat((x, y), 1),\n",
        "            'mult': lambda x, y: x * y,\n",
        "            'sub': lambda x, y: x - y\n",
        "        }\n",
        "\n",
        "    def forward(self, c1, c2, lengths):\n",
        "        # reset hidden state\n",
        "        #print(c1.size())\n",
        "        batch_size, seq_len, emb_size = c1.size()\n",
        "\n",
        "        #print(self.hidden_1.size())\n",
        "        # get embedding of characters\n",
        "        c1_hidden = self.conv1(c1.transpose(1,2)).transpose(1,2)\n",
        "        c1_hidden = F.relu(c1_hidden.contiguous().view(-1, c1_hidden.size(-1))).view(batch_size, seq_len, c1_hidden.size(-1))\n",
        "        c1_hidden = self.conv2(c1_hidden.transpose(1,2)).transpose(1,2)\n",
        "        c1_hidden = F.relu(c1_hidden.contiguous().view(-1, c1_hidden.size(-1))).view(batch_size, seq_len, c1_hidden.size(-1))\n",
        "        \n",
        "        c2_hidden = self.conv1(c2.transpose(1,2)).transpose(1,2)\n",
        "        c2_hidden = F.relu(c2_hidden.contiguous().view(-1, c2_hidden.size(-1))).view(batch_size, seq_len, c2_hidden.size(-1))\n",
        "        c2_hidden = self.conv2(c2_hidden.transpose(1,2)).transpose(1,2)\n",
        "        c2_hidden = F.relu(c2_hidden.contiguous().view(-1, c2_hidden.size(-1))).view(batch_size, seq_len, c2_hidden.size(-1))\n",
        "        \n",
        "        #Concat, max pool\n",
        "        concated_hidden = self.interact_method[self.interact](c1_hidden, c2_hidden)\n",
        "        concated_hidden, _ = torch.max(concated_hidden, dim=2)\n",
        "        #print('embed_size:',embed.size())\n",
        "        # pack padded sequence\n",
        "        \n",
        "        # fprop though RNN\n",
        "        \n",
        "        # print('rnn_out_size:',rnn_out.size())\n",
        "        # print('hidden_size:',self.hidden.size())\n",
        "        # undo packing\n",
        "        \n",
        "        l1_out = self.linear_1(concated_hidden)\n",
        "        l1_out = F.relu(l1_out.contiguous())\n",
        "        l2_out = self.linear_2(l1_out)\n",
        "        #print(l2_out.size())\n",
        "        #print('rnn_out_size_2:',rnn_out.size())\n",
        "        # sum hidden activations of RNN across time\n",
        "        #rnn_out = torch.sum(rnn_out, dim=1)\n",
        "        #print('rnn_out_size_final:',rnn_out.size())\n",
        "        #logits = self.linear(rnn_out)\n",
        "        #print('logits_size_final:',logits.size())\n",
        "        #print(l2_out.size())\n",
        "        return l2_out.cuda()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k_IUFOS8xEqj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_model(loader, model):\n",
        "    \"\"\"\n",
        "    Help function that tests the model's performance on a dataset\n",
        "    @param: loader - data loader for the dataset to test against\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    for i, (cen_1, cen_2, lengths, labels) in enumerate(loader):\n",
        "        cen_1_batch, cen_2_batch, lengths_batch, label_batch = cen_1.float().cuda(), cen_2.float().cuda(), lengths.cuda(), labels.cuda()\n",
        "        outputs = F.softmax(model(cen_1_batch, cen_2_batch, lengths_batch), dim=1).cuda()\n",
        "        predicted = outputs.max(1, keepdim=True)[1]\n",
        "        predicted = predicted.cuda()\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels.view_as(predicted).cuda()).sum().item()\n",
        "    return (100 * correct / total)\n",
        "\n",
        "def model_runner_cnn(emb_size=300, hidden_size=200, num_layers=1, num_classes=3, kernel_size=5, padding_size=2, \\\n",
        "                     interact='concat', weight_decay = 0, learning_rate=2e-4, num_epochs=5, early_terminate=False):\n",
        "    if 'model' in dir():\n",
        "        del(model)\n",
        "        \n",
        "    model = CNN(emb_size=emb_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes, \\\n",
        "                kernel_size=kernel_size, padding_size=padding_size, len_sen=78)\n",
        "\n",
        "    learning_rate = learning_rate\n",
        "    num_epochs = num_epochs # number epoch to train\n",
        "\n",
        "    # Criterion and Optimizer\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay = weight_decay)\n",
        "    model.cuda()\n",
        "    criterion.cuda()\n",
        "\n",
        "    print('num_parameters:', sum(p.numel() for p in model.parameters()))\n",
        "    \n",
        "    # Train the model\n",
        "    total_step = len(train_loader)\n",
        "\n",
        "    train_loss_list = []\n",
        "    val_acc_list = []\n",
        "    earlyT = False\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        if early_terminate == True and earlyT == True:\n",
        "            break\n",
        "        for i, (cen_1, cen_2, lengths, labels) in enumerate(train_loader):\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            outputs = model(cen_1.float().cuda(), cen_2.float().cuda(), lengths.cuda())\n",
        "            #print(outputs.size())\n",
        "            #print(labels.size())\n",
        "            loss = criterion(outputs, labels.cuda())\n",
        "\n",
        "            # Backward and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if i > 0 and i % 300 == 0:\n",
        "                # validate\n",
        "                correct = 0\n",
        "                total = 0\n",
        "                outputs = F.softmax(model(cen_1.float().cuda(), cen_2.float().cuda(), lengths.cuda()), dim=1).cuda()\n",
        "                predicted = outputs.max(1, keepdim=True)[1]\n",
        "                predicted = predicted.cuda()\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels.view_as(predicted).cuda()).sum().item()\n",
        "                train_loss = 100 * correct/total\n",
        "                train_loss_list.append(train_loss)\n",
        "                val_acc = test_model(val_loader, model)\n",
        "                val_acc_list.append(val_acc)\n",
        "                print('Epoch: [{}/{}], Step: [{}/{}], Train Acc: {}, Val Acc: {}'.format(\n",
        "                           epoch+1, num_epochs, i+1, len(train_loader), train_loss, val_acc))\n",
        "                # if val_acc > 65 and early_terminate == True:\n",
        "                #     earlyT = True\n",
        "                #     break\n",
        "    pkl_dumper(train_loss_list, resPath + '/train_ACC_list_' + str(hidden_size) + '_' + str(kernel_size) + '_' + str(interact) + str(learning_rate) + str(weight_decay) + '.p')\n",
        "    pkl_dumper(val_acc_list, resPath + '/val_acc_list_' + str(hidden_size) + '_' + str(kernel_size) + '_' + str(interact) + str(learning_rate) + str(weight_decay) + '.p')\n",
        "    torch.save(model.state_dict(), resPath + '/model_' + str(hidden_size) + '_' + str(kernel_size) + '_' + str(interact) + str(learning_rate) + str(weight_decay) + '.model')\n",
        "    return(model, train_loss_list, val_acc_list)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dpQBiEhHPDS3",
        "colab_type": "code",
        "outputId": "a7113b83-3112-4860-f3b5-50129a00541e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1224
        }
      },
      "cell_type": "code",
      "source": [
        "model, train_loss_cnn1, val_acc_cnn1 = model_runner_cnn(num_epochs = 7)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_parameters: 512883\n",
            "Epoch: [1/7], Step: [301/3125], Train Acc: 40.625, Val Acc: 45.4\n",
            "Epoch: [1/7], Step: [601/3125], Train Acc: 40.625, Val Acc: 49.9\n",
            "Epoch: [1/7], Step: [901/3125], Train Acc: 43.75, Val Acc: 51.6\n",
            "Epoch: [1/7], Step: [1201/3125], Train Acc: 50.0, Val Acc: 52.5\n",
            "Epoch: [1/7], Step: [1501/3125], Train Acc: 46.875, Val Acc: 53.9\n",
            "Epoch: [1/7], Step: [1801/3125], Train Acc: 53.125, Val Acc: 54.4\n",
            "Epoch: [1/7], Step: [2101/3125], Train Acc: 71.875, Val Acc: 55.5\n",
            "Epoch: [1/7], Step: [2401/3125], Train Acc: 59.375, Val Acc: 57.1\n",
            "Epoch: [1/7], Step: [2701/3125], Train Acc: 68.75, Val Acc: 57.2\n",
            "Epoch: [1/7], Step: [3001/3125], Train Acc: 62.5, Val Acc: 57.5\n",
            "Epoch: [2/7], Step: [301/3125], Train Acc: 68.75, Val Acc: 57.2\n",
            "Epoch: [2/7], Step: [601/3125], Train Acc: 62.5, Val Acc: 58.0\n",
            "Epoch: [2/7], Step: [901/3125], Train Acc: 59.375, Val Acc: 57.6\n",
            "Epoch: [2/7], Step: [1201/3125], Train Acc: 56.25, Val Acc: 58.6\n",
            "Epoch: [2/7], Step: [1501/3125], Train Acc: 50.0, Val Acc: 59.6\n",
            "Epoch: [2/7], Step: [1801/3125], Train Acc: 53.125, Val Acc: 59.9\n",
            "Epoch: [2/7], Step: [2101/3125], Train Acc: 53.125, Val Acc: 58.9\n",
            "Epoch: [2/7], Step: [2401/3125], Train Acc: 59.375, Val Acc: 60.2\n",
            "Epoch: [2/7], Step: [2701/3125], Train Acc: 84.375, Val Acc: 61.0\n",
            "Epoch: [2/7], Step: [3001/3125], Train Acc: 71.875, Val Acc: 60.2\n",
            "Epoch: [3/7], Step: [301/3125], Train Acc: 65.625, Val Acc: 60.8\n",
            "Epoch: [3/7], Step: [601/3125], Train Acc: 65.625, Val Acc: 60.5\n",
            "Epoch: [3/7], Step: [901/3125], Train Acc: 65.625, Val Acc: 60.2\n",
            "Epoch: [3/7], Step: [1201/3125], Train Acc: 65.625, Val Acc: 61.1\n",
            "Epoch: [3/7], Step: [1501/3125], Train Acc: 40.625, Val Acc: 60.7\n",
            "Epoch: [3/7], Step: [1801/3125], Train Acc: 46.875, Val Acc: 61.1\n",
            "Epoch: [3/7], Step: [2101/3125], Train Acc: 50.0, Val Acc: 61.6\n",
            "Epoch: [3/7], Step: [2401/3125], Train Acc: 53.125, Val Acc: 61.3\n",
            "Epoch: [3/7], Step: [2701/3125], Train Acc: 62.5, Val Acc: 61.7\n",
            "Epoch: [3/7], Step: [3001/3125], Train Acc: 75.0, Val Acc: 61.3\n",
            "Epoch: [4/7], Step: [301/3125], Train Acc: 65.625, Val Acc: 61.9\n",
            "Epoch: [4/7], Step: [601/3125], Train Acc: 65.625, Val Acc: 63.4\n",
            "Epoch: [4/7], Step: [901/3125], Train Acc: 68.75, Val Acc: 61.7\n",
            "Epoch: [4/7], Step: [1201/3125], Train Acc: 68.75, Val Acc: 62.1\n",
            "Epoch: [4/7], Step: [1501/3125], Train Acc: 68.75, Val Acc: 60.9\n",
            "Epoch: [4/7], Step: [1801/3125], Train Acc: 56.25, Val Acc: 61.3\n",
            "Epoch: [4/7], Step: [2101/3125], Train Acc: 62.5, Val Acc: 62.2\n",
            "Epoch: [4/7], Step: [2401/3125], Train Acc: 68.75, Val Acc: 61.9\n",
            "Epoch: [4/7], Step: [2701/3125], Train Acc: 56.25, Val Acc: 62.6\n",
            "Epoch: [4/7], Step: [3001/3125], Train Acc: 75.0, Val Acc: 62.4\n",
            "Epoch: [5/7], Step: [301/3125], Train Acc: 62.5, Val Acc: 61.9\n",
            "Epoch: [5/7], Step: [601/3125], Train Acc: 68.75, Val Acc: 63.7\n",
            "Epoch: [5/7], Step: [901/3125], Train Acc: 81.25, Val Acc: 62.3\n",
            "Epoch: [5/7], Step: [1201/3125], Train Acc: 68.75, Val Acc: 61.8\n",
            "Epoch: [5/7], Step: [1501/3125], Train Acc: 78.125, Val Acc: 63.5\n",
            "Epoch: [5/7], Step: [1801/3125], Train Acc: 62.5, Val Acc: 62.2\n",
            "Epoch: [5/7], Step: [2101/3125], Train Acc: 59.375, Val Acc: 64.2\n",
            "Epoch: [5/7], Step: [2401/3125], Train Acc: 68.75, Val Acc: 62.0\n",
            "Epoch: [5/7], Step: [2701/3125], Train Acc: 78.125, Val Acc: 63.2\n",
            "Epoch: [5/7], Step: [3001/3125], Train Acc: 65.625, Val Acc: 63.0\n",
            "Epoch: [6/7], Step: [301/3125], Train Acc: 59.375, Val Acc: 63.9\n",
            "Epoch: [6/7], Step: [601/3125], Train Acc: 71.875, Val Acc: 63.2\n",
            "Epoch: [6/7], Step: [901/3125], Train Acc: 75.0, Val Acc: 64.8\n",
            "Epoch: [6/7], Step: [1201/3125], Train Acc: 71.875, Val Acc: 64.5\n",
            "Epoch: [6/7], Step: [1501/3125], Train Acc: 68.75, Val Acc: 62.3\n",
            "Epoch: [6/7], Step: [1801/3125], Train Acc: 62.5, Val Acc: 63.0\n",
            "Epoch: [6/7], Step: [2101/3125], Train Acc: 71.875, Val Acc: 63.6\n",
            "Epoch: [6/7], Step: [2401/3125], Train Acc: 68.75, Val Acc: 63.4\n",
            "Epoch: [6/7], Step: [2701/3125], Train Acc: 65.625, Val Acc: 64.3\n",
            "Epoch: [6/7], Step: [3001/3125], Train Acc: 65.625, Val Acc: 63.3\n",
            "Epoch: [7/7], Step: [301/3125], Train Acc: 71.875, Val Acc: 64.6\n",
            "Epoch: [7/7], Step: [601/3125], Train Acc: 84.375, Val Acc: 65.9\n",
            "Epoch: [7/7], Step: [901/3125], Train Acc: 68.75, Val Acc: 65.0\n",
            "Epoch: [7/7], Step: [1201/3125], Train Acc: 56.25, Val Acc: 64.4\n",
            "Epoch: [7/7], Step: [1501/3125], Train Acc: 68.75, Val Acc: 64.2\n",
            "Epoch: [7/7], Step: [1801/3125], Train Acc: 81.25, Val Acc: 63.1\n",
            "Epoch: [7/7], Step: [2101/3125], Train Acc: 81.25, Val Acc: 63.1\n",
            "Epoch: [7/7], Step: [2401/3125], Train Acc: 71.875, Val Acc: 63.3\n",
            "Epoch: [7/7], Step: [2701/3125], Train Acc: 75.0, Val Acc: 62.8\n",
            "Epoch: [7/7], Step: [3001/3125], Train Acc: 84.375, Val Acc: 64.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "o_7eY13UBSAL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model, train_loss_cnn1, val_acc_cnn1 = model_runner_cnn(num_epochs = 7, weight_decay = 0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NUPSbVJbWuH0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1224
        },
        "outputId": "aab6c0b3-99b8-41fa-d463-5a9fe21bb006"
      },
      "cell_type": "code",
      "source": [
        "model2, train_loss_cnn2, val_acc_cnn2 = model_runner_cnn(num_epochs = 7, hidden_size = 300, kernel_size = 3, padding_size = 1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_parameters: 553083\n",
            "Epoch: [1/7], Step: [301/3125], Train Acc: 25.0, Val Acc: 40.7\n",
            "Epoch: [1/7], Step: [601/3125], Train Acc: 40.625, Val Acc: 44.8\n",
            "Epoch: [1/7], Step: [901/3125], Train Acc: 53.125, Val Acc: 49.0\n",
            "Epoch: [1/7], Step: [1201/3125], Train Acc: 53.125, Val Acc: 52.4\n",
            "Epoch: [1/7], Step: [1501/3125], Train Acc: 62.5, Val Acc: 51.6\n",
            "Epoch: [1/7], Step: [1801/3125], Train Acc: 43.75, Val Acc: 54.6\n",
            "Epoch: [1/7], Step: [2101/3125], Train Acc: 65.625, Val Acc: 53.0\n",
            "Epoch: [1/7], Step: [2401/3125], Train Acc: 46.875, Val Acc: 55.0\n",
            "Epoch: [1/7], Step: [2701/3125], Train Acc: 62.5, Val Acc: 57.0\n",
            "Epoch: [1/7], Step: [3001/3125], Train Acc: 62.5, Val Acc: 56.2\n",
            "Epoch: [2/7], Step: [301/3125], Train Acc: 59.375, Val Acc: 57.2\n",
            "Epoch: [2/7], Step: [601/3125], Train Acc: 75.0, Val Acc: 58.4\n",
            "Epoch: [2/7], Step: [901/3125], Train Acc: 59.375, Val Acc: 59.1\n",
            "Epoch: [2/7], Step: [1201/3125], Train Acc: 62.5, Val Acc: 59.1\n",
            "Epoch: [2/7], Step: [1501/3125], Train Acc: 65.625, Val Acc: 58.6\n",
            "Epoch: [2/7], Step: [1801/3125], Train Acc: 53.125, Val Acc: 59.2\n",
            "Epoch: [2/7], Step: [2101/3125], Train Acc: 65.625, Val Acc: 59.1\n",
            "Epoch: [2/7], Step: [2401/3125], Train Acc: 62.5, Val Acc: 58.4\n",
            "Epoch: [2/7], Step: [2701/3125], Train Acc: 59.375, Val Acc: 59.7\n",
            "Epoch: [2/7], Step: [3001/3125], Train Acc: 50.0, Val Acc: 58.6\n",
            "Epoch: [3/7], Step: [301/3125], Train Acc: 59.375, Val Acc: 58.8\n",
            "Epoch: [3/7], Step: [601/3125], Train Acc: 62.5, Val Acc: 59.7\n",
            "Epoch: [3/7], Step: [901/3125], Train Acc: 71.875, Val Acc: 58.9\n",
            "Epoch: [3/7], Step: [1201/3125], Train Acc: 68.75, Val Acc: 59.9\n",
            "Epoch: [3/7], Step: [1501/3125], Train Acc: 68.75, Val Acc: 60.9\n",
            "Epoch: [3/7], Step: [1801/3125], Train Acc: 65.625, Val Acc: 60.8\n",
            "Epoch: [3/7], Step: [2101/3125], Train Acc: 68.75, Val Acc: 61.3\n",
            "Epoch: [3/7], Step: [2401/3125], Train Acc: 71.875, Val Acc: 61.2\n",
            "Epoch: [3/7], Step: [2701/3125], Train Acc: 71.875, Val Acc: 61.4\n",
            "Epoch: [3/7], Step: [3001/3125], Train Acc: 75.0, Val Acc: 60.8\n",
            "Epoch: [4/7], Step: [301/3125], Train Acc: 71.875, Val Acc: 61.3\n",
            "Epoch: [4/7], Step: [601/3125], Train Acc: 62.5, Val Acc: 61.2\n",
            "Epoch: [4/7], Step: [901/3125], Train Acc: 65.625, Val Acc: 60.2\n",
            "Epoch: [4/7], Step: [1201/3125], Train Acc: 65.625, Val Acc: 61.6\n",
            "Epoch: [4/7], Step: [1501/3125], Train Acc: 50.0, Val Acc: 61.6\n",
            "Epoch: [4/7], Step: [1801/3125], Train Acc: 75.0, Val Acc: 61.4\n",
            "Epoch: [4/7], Step: [2101/3125], Train Acc: 53.125, Val Acc: 62.3\n",
            "Epoch: [4/7], Step: [2401/3125], Train Acc: 71.875, Val Acc: 60.9\n",
            "Epoch: [4/7], Step: [2701/3125], Train Acc: 62.5, Val Acc: 62.2\n",
            "Epoch: [4/7], Step: [3001/3125], Train Acc: 71.875, Val Acc: 61.4\n",
            "Epoch: [5/7], Step: [301/3125], Train Acc: 53.125, Val Acc: 62.0\n",
            "Epoch: [5/7], Step: [601/3125], Train Acc: 78.125, Val Acc: 61.4\n",
            "Epoch: [5/7], Step: [901/3125], Train Acc: 62.5, Val Acc: 61.8\n",
            "Epoch: [5/7], Step: [1201/3125], Train Acc: 56.25, Val Acc: 62.1\n",
            "Epoch: [5/7], Step: [1501/3125], Train Acc: 71.875, Val Acc: 61.5\n",
            "Epoch: [5/7], Step: [1801/3125], Train Acc: 84.375, Val Acc: 62.3\n",
            "Epoch: [5/7], Step: [2101/3125], Train Acc: 75.0, Val Acc: 62.1\n",
            "Epoch: [5/7], Step: [2401/3125], Train Acc: 62.5, Val Acc: 62.9\n",
            "Epoch: [5/7], Step: [2701/3125], Train Acc: 56.25, Val Acc: 61.9\n",
            "Epoch: [5/7], Step: [3001/3125], Train Acc: 71.875, Val Acc: 62.9\n",
            "Epoch: [6/7], Step: [301/3125], Train Acc: 84.375, Val Acc: 62.6\n",
            "Epoch: [6/7], Step: [601/3125], Train Acc: 71.875, Val Acc: 62.2\n",
            "Epoch: [6/7], Step: [901/3125], Train Acc: 65.625, Val Acc: 62.0\n",
            "Epoch: [6/7], Step: [1201/3125], Train Acc: 68.75, Val Acc: 62.7\n",
            "Epoch: [6/7], Step: [1501/3125], Train Acc: 71.875, Val Acc: 62.9\n",
            "Epoch: [6/7], Step: [1801/3125], Train Acc: 68.75, Val Acc: 62.5\n",
            "Epoch: [6/7], Step: [2101/3125], Train Acc: 65.625, Val Acc: 63.3\n",
            "Epoch: [6/7], Step: [2401/3125], Train Acc: 65.625, Val Acc: 62.7\n",
            "Epoch: [6/7], Step: [2701/3125], Train Acc: 62.5, Val Acc: 62.6\n",
            "Epoch: [6/7], Step: [3001/3125], Train Acc: 87.5, Val Acc: 62.0\n",
            "Epoch: [7/7], Step: [301/3125], Train Acc: 68.75, Val Acc: 62.9\n",
            "Epoch: [7/7], Step: [601/3125], Train Acc: 59.375, Val Acc: 62.7\n",
            "Epoch: [7/7], Step: [901/3125], Train Acc: 59.375, Val Acc: 61.6\n",
            "Epoch: [7/7], Step: [1201/3125], Train Acc: 75.0, Val Acc: 62.5\n",
            "Epoch: [7/7], Step: [1501/3125], Train Acc: 68.75, Val Acc: 62.1\n",
            "Epoch: [7/7], Step: [1801/3125], Train Acc: 78.125, Val Acc: 62.3\n",
            "Epoch: [7/7], Step: [2101/3125], Train Acc: 59.375, Val Acc: 61.4\n",
            "Epoch: [7/7], Step: [2401/3125], Train Acc: 62.5, Val Acc: 62.2\n",
            "Epoch: [7/7], Step: [2701/3125], Train Acc: 68.75, Val Acc: 62.6\n",
            "Epoch: [7/7], Step: [3001/3125], Train Acc: 59.375, Val Acc: 63.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nRKGDRJ6PJqR",
        "colab_type": "code",
        "outputId": "89b5aef8-6eb9-4ee4-9b0b-664d24574f0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1224
        }
      },
      "cell_type": "code",
      "source": [
        "#add weight decay\n",
        "model, train_loss_cnn1, val_acc_cnn1 = model_runner_cnn(num_epochs = 7, hidden_size = 300, kernel_size = 7, padding_size = 3)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_parameters: 1273083\n",
            "Epoch: [1/7], Step: [301/3125], Train Acc: 43.75, Val Acc: 46.2\n",
            "Epoch: [1/7], Step: [601/3125], Train Acc: 43.75, Val Acc: 50.7\n",
            "Epoch: [1/7], Step: [901/3125], Train Acc: 59.375, Val Acc: 52.5\n",
            "Epoch: [1/7], Step: [1201/3125], Train Acc: 68.75, Val Acc: 55.1\n",
            "Epoch: [1/7], Step: [1501/3125], Train Acc: 62.5, Val Acc: 55.2\n",
            "Epoch: [1/7], Step: [1801/3125], Train Acc: 62.5, Val Acc: 56.5\n",
            "Epoch: [1/7], Step: [2101/3125], Train Acc: 65.625, Val Acc: 57.2\n",
            "Epoch: [1/7], Step: [2401/3125], Train Acc: 50.0, Val Acc: 58.3\n",
            "Epoch: [1/7], Step: [2701/3125], Train Acc: 62.5, Val Acc: 57.6\n",
            "Epoch: [1/7], Step: [3001/3125], Train Acc: 68.75, Val Acc: 59.0\n",
            "Epoch: [2/7], Step: [301/3125], Train Acc: 56.25, Val Acc: 58.4\n",
            "Epoch: [2/7], Step: [601/3125], Train Acc: 56.25, Val Acc: 59.1\n",
            "Epoch: [2/7], Step: [901/3125], Train Acc: 56.25, Val Acc: 60.1\n",
            "Epoch: [2/7], Step: [1201/3125], Train Acc: 56.25, Val Acc: 60.8\n",
            "Epoch: [2/7], Step: [1501/3125], Train Acc: 62.5, Val Acc: 59.9\n",
            "Epoch: [2/7], Step: [1801/3125], Train Acc: 59.375, Val Acc: 61.4\n",
            "Epoch: [2/7], Step: [2101/3125], Train Acc: 62.5, Val Acc: 60.3\n",
            "Epoch: [2/7], Step: [2401/3125], Train Acc: 68.75, Val Acc: 60.9\n",
            "Epoch: [2/7], Step: [2701/3125], Train Acc: 68.75, Val Acc: 60.4\n",
            "Epoch: [2/7], Step: [3001/3125], Train Acc: 50.0, Val Acc: 61.3\n",
            "Epoch: [3/7], Step: [301/3125], Train Acc: 62.5, Val Acc: 61.6\n",
            "Epoch: [3/7], Step: [601/3125], Train Acc: 75.0, Val Acc: 59.6\n",
            "Epoch: [3/7], Step: [901/3125], Train Acc: 62.5, Val Acc: 60.8\n",
            "Epoch: [3/7], Step: [1201/3125], Train Acc: 62.5, Val Acc: 60.7\n",
            "Epoch: [3/7], Step: [1501/3125], Train Acc: 59.375, Val Acc: 61.8\n",
            "Epoch: [3/7], Step: [1801/3125], Train Acc: 62.5, Val Acc: 61.4\n",
            "Epoch: [3/7], Step: [2101/3125], Train Acc: 62.5, Val Acc: 61.1\n",
            "Epoch: [3/7], Step: [2401/3125], Train Acc: 50.0, Val Acc: 62.7\n",
            "Epoch: [3/7], Step: [2701/3125], Train Acc: 68.75, Val Acc: 62.5\n",
            "Epoch: [3/7], Step: [3001/3125], Train Acc: 71.875, Val Acc: 63.9\n",
            "Epoch: [4/7], Step: [301/3125], Train Acc: 71.875, Val Acc: 61.4\n",
            "Epoch: [4/7], Step: [601/3125], Train Acc: 71.875, Val Acc: 62.9\n",
            "Epoch: [4/7], Step: [901/3125], Train Acc: 53.125, Val Acc: 61.7\n",
            "Epoch: [4/7], Step: [1201/3125], Train Acc: 78.125, Val Acc: 61.0\n",
            "Epoch: [4/7], Step: [1501/3125], Train Acc: 68.75, Val Acc: 62.5\n",
            "Epoch: [4/7], Step: [1801/3125], Train Acc: 56.25, Val Acc: 64.2\n",
            "Epoch: [4/7], Step: [2101/3125], Train Acc: 56.25, Val Acc: 64.4\n",
            "Epoch: [4/7], Step: [2401/3125], Train Acc: 75.0, Val Acc: 62.4\n",
            "Epoch: [4/7], Step: [2701/3125], Train Acc: 71.875, Val Acc: 64.5\n",
            "Epoch: [4/7], Step: [3001/3125], Train Acc: 53.125, Val Acc: 63.4\n",
            "Epoch: [5/7], Step: [301/3125], Train Acc: 78.125, Val Acc: 62.4\n",
            "Epoch: [5/7], Step: [601/3125], Train Acc: 71.875, Val Acc: 62.8\n",
            "Epoch: [5/7], Step: [901/3125], Train Acc: 65.625, Val Acc: 62.3\n",
            "Epoch: [5/7], Step: [1201/3125], Train Acc: 71.875, Val Acc: 62.9\n",
            "Epoch: [5/7], Step: [1501/3125], Train Acc: 68.75, Val Acc: 64.0\n",
            "Epoch: [5/7], Step: [1801/3125], Train Acc: 78.125, Val Acc: 61.3\n",
            "Epoch: [5/7], Step: [2101/3125], Train Acc: 78.125, Val Acc: 63.4\n",
            "Epoch: [5/7], Step: [2401/3125], Train Acc: 71.875, Val Acc: 63.2\n",
            "Epoch: [5/7], Step: [2701/3125], Train Acc: 87.5, Val Acc: 63.9\n",
            "Epoch: [5/7], Step: [3001/3125], Train Acc: 75.0, Val Acc: 64.0\n",
            "Epoch: [6/7], Step: [301/3125], Train Acc: 68.75, Val Acc: 62.7\n",
            "Epoch: [6/7], Step: [601/3125], Train Acc: 68.75, Val Acc: 63.1\n",
            "Epoch: [6/7], Step: [901/3125], Train Acc: 75.0, Val Acc: 61.4\n",
            "Epoch: [6/7], Step: [1201/3125], Train Acc: 84.375, Val Acc: 62.7\n",
            "Epoch: [6/7], Step: [1501/3125], Train Acc: 75.0, Val Acc: 62.5\n",
            "Epoch: [6/7], Step: [1801/3125], Train Acc: 87.5, Val Acc: 62.6\n",
            "Epoch: [6/7], Step: [2101/3125], Train Acc: 81.25, Val Acc: 63.5\n",
            "Epoch: [6/7], Step: [2401/3125], Train Acc: 75.0, Val Acc: 61.6\n",
            "Epoch: [6/7], Step: [2701/3125], Train Acc: 68.75, Val Acc: 63.7\n",
            "Epoch: [6/7], Step: [3001/3125], Train Acc: 65.625, Val Acc: 60.4\n",
            "Epoch: [7/7], Step: [301/3125], Train Acc: 81.25, Val Acc: 61.5\n",
            "Epoch: [7/7], Step: [601/3125], Train Acc: 78.125, Val Acc: 61.8\n",
            "Epoch: [7/7], Step: [901/3125], Train Acc: 84.375, Val Acc: 62.6\n",
            "Epoch: [7/7], Step: [1201/3125], Train Acc: 78.125, Val Acc: 62.6\n",
            "Epoch: [7/7], Step: [1501/3125], Train Acc: 71.875, Val Acc: 61.1\n",
            "Epoch: [7/7], Step: [1801/3125], Train Acc: 81.25, Val Acc: 61.8\n",
            "Epoch: [7/7], Step: [2101/3125], Train Acc: 84.375, Val Acc: 61.8\n",
            "Epoch: [7/7], Step: [2401/3125], Train Acc: 75.0, Val Acc: 61.1\n",
            "Epoch: [7/7], Step: [2701/3125], Train Acc: 84.375, Val Acc: 61.3\n",
            "Epoch: [7/7], Step: [3001/3125], Train Acc: 90.625, Val Acc: 62.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5j_PyanFU2AT",
        "colab_type": "code",
        "outputId": "4786e596-2ffa-4736-abf1-5fbae952ec10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1224
        }
      },
      "cell_type": "code",
      "source": [
        "model2, train_loss_cnn2, val_acc_cnn2 = model_runner_cnn(num_epochs = 7, hidden_size = 300)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_parameters: 913083\n",
            "Epoch: [1/7], Step: [301/3125], Train Acc: 59.375, Val Acc: 43.3\n",
            "Epoch: [1/7], Step: [601/3125], Train Acc: 75.0, Val Acc: 48.8\n",
            "Epoch: [1/7], Step: [901/3125], Train Acc: 62.5, Val Acc: 52.2\n",
            "Epoch: [1/7], Step: [1201/3125], Train Acc: 50.0, Val Acc: 52.5\n",
            "Epoch: [1/7], Step: [1501/3125], Train Acc: 37.5, Val Acc: 55.7\n",
            "Epoch: [1/7], Step: [1801/3125], Train Acc: 68.75, Val Acc: 56.9\n",
            "Epoch: [1/7], Step: [2101/3125], Train Acc: 68.75, Val Acc: 57.8\n",
            "Epoch: [1/7], Step: [2401/3125], Train Acc: 78.125, Val Acc: 57.7\n",
            "Epoch: [1/7], Step: [2701/3125], Train Acc: 53.125, Val Acc: 57.3\n",
            "Epoch: [1/7], Step: [3001/3125], Train Acc: 62.5, Val Acc: 56.3\n",
            "Epoch: [2/7], Step: [301/3125], Train Acc: 62.5, Val Acc: 58.0\n",
            "Epoch: [2/7], Step: [601/3125], Train Acc: 65.625, Val Acc: 58.9\n",
            "Epoch: [2/7], Step: [901/3125], Train Acc: 62.5, Val Acc: 58.1\n",
            "Epoch: [2/7], Step: [1201/3125], Train Acc: 62.5, Val Acc: 59.4\n",
            "Epoch: [2/7], Step: [1501/3125], Train Acc: 71.875, Val Acc: 60.2\n",
            "Epoch: [2/7], Step: [1801/3125], Train Acc: 59.375, Val Acc: 59.9\n",
            "Epoch: [2/7], Step: [2101/3125], Train Acc: 53.125, Val Acc: 60.6\n",
            "Epoch: [2/7], Step: [2401/3125], Train Acc: 46.875, Val Acc: 60.2\n",
            "Epoch: [2/7], Step: [2701/3125], Train Acc: 50.0, Val Acc: 60.8\n",
            "Epoch: [2/7], Step: [3001/3125], Train Acc: 53.125, Val Acc: 60.6\n",
            "Epoch: [3/7], Step: [301/3125], Train Acc: 68.75, Val Acc: 60.6\n",
            "Epoch: [3/7], Step: [601/3125], Train Acc: 59.375, Val Acc: 61.1\n",
            "Epoch: [3/7], Step: [901/3125], Train Acc: 59.375, Val Acc: 60.7\n",
            "Epoch: [3/7], Step: [1201/3125], Train Acc: 65.625, Val Acc: 60.4\n",
            "Epoch: [3/7], Step: [1501/3125], Train Acc: 56.25, Val Acc: 60.8\n",
            "Epoch: [3/7], Step: [1801/3125], Train Acc: 71.875, Val Acc: 61.2\n",
            "Epoch: [3/7], Step: [2101/3125], Train Acc: 71.875, Val Acc: 61.4\n",
            "Epoch: [3/7], Step: [2401/3125], Train Acc: 59.375, Val Acc: 63.5\n",
            "Epoch: [3/7], Step: [2701/3125], Train Acc: 56.25, Val Acc: 62.2\n",
            "Epoch: [3/7], Step: [3001/3125], Train Acc: 75.0, Val Acc: 64.5\n",
            "Epoch: [4/7], Step: [301/3125], Train Acc: 71.875, Val Acc: 62.3\n",
            "Epoch: [4/7], Step: [601/3125], Train Acc: 53.125, Val Acc: 61.8\n",
            "Epoch: [4/7], Step: [901/3125], Train Acc: 75.0, Val Acc: 64.4\n",
            "Epoch: [4/7], Step: [1201/3125], Train Acc: 78.125, Val Acc: 61.1\n",
            "Epoch: [4/7], Step: [1501/3125], Train Acc: 75.0, Val Acc: 62.5\n",
            "Epoch: [4/7], Step: [1801/3125], Train Acc: 75.0, Val Acc: 62.0\n",
            "Epoch: [4/7], Step: [2101/3125], Train Acc: 62.5, Val Acc: 63.3\n",
            "Epoch: [4/7], Step: [2401/3125], Train Acc: 71.875, Val Acc: 62.5\n",
            "Epoch: [4/7], Step: [2701/3125], Train Acc: 75.0, Val Acc: 62.1\n",
            "Epoch: [4/7], Step: [3001/3125], Train Acc: 84.375, Val Acc: 63.8\n",
            "Epoch: [5/7], Step: [301/3125], Train Acc: 65.625, Val Acc: 64.3\n",
            "Epoch: [5/7], Step: [601/3125], Train Acc: 65.625, Val Acc: 63.3\n",
            "Epoch: [5/7], Step: [901/3125], Train Acc: 68.75, Val Acc: 63.5\n",
            "Epoch: [5/7], Step: [1201/3125], Train Acc: 90.625, Val Acc: 63.0\n",
            "Epoch: [5/7], Step: [1501/3125], Train Acc: 71.875, Val Acc: 62.5\n",
            "Epoch: [5/7], Step: [1801/3125], Train Acc: 62.5, Val Acc: 63.4\n",
            "Epoch: [5/7], Step: [2101/3125], Train Acc: 71.875, Val Acc: 63.4\n",
            "Epoch: [5/7], Step: [2401/3125], Train Acc: 84.375, Val Acc: 63.5\n",
            "Epoch: [5/7], Step: [2701/3125], Train Acc: 71.875, Val Acc: 64.2\n",
            "Epoch: [5/7], Step: [3001/3125], Train Acc: 59.375, Val Acc: 63.8\n",
            "Epoch: [6/7], Step: [301/3125], Train Acc: 68.75, Val Acc: 63.6\n",
            "Epoch: [6/7], Step: [601/3125], Train Acc: 71.875, Val Acc: 63.8\n",
            "Epoch: [6/7], Step: [901/3125], Train Acc: 75.0, Val Acc: 62.3\n",
            "Epoch: [6/7], Step: [1201/3125], Train Acc: 56.25, Val Acc: 62.9\n",
            "Epoch: [6/7], Step: [1501/3125], Train Acc: 68.75, Val Acc: 63.0\n",
            "Epoch: [6/7], Step: [1801/3125], Train Acc: 65.625, Val Acc: 61.7\n",
            "Epoch: [6/7], Step: [2101/3125], Train Acc: 75.0, Val Acc: 62.0\n",
            "Epoch: [6/7], Step: [2401/3125], Train Acc: 68.75, Val Acc: 61.4\n",
            "Epoch: [6/7], Step: [2701/3125], Train Acc: 65.625, Val Acc: 63.5\n",
            "Epoch: [6/7], Step: [3001/3125], Train Acc: 71.875, Val Acc: 63.3\n",
            "Epoch: [7/7], Step: [301/3125], Train Acc: 71.875, Val Acc: 63.2\n",
            "Epoch: [7/7], Step: [601/3125], Train Acc: 68.75, Val Acc: 63.2\n",
            "Epoch: [7/7], Step: [901/3125], Train Acc: 71.875, Val Acc: 63.0\n",
            "Epoch: [7/7], Step: [1201/3125], Train Acc: 71.875, Val Acc: 63.3\n",
            "Epoch: [7/7], Step: [1501/3125], Train Acc: 81.25, Val Acc: 63.6\n",
            "Epoch: [7/7], Step: [1801/3125], Train Acc: 62.5, Val Acc: 63.3\n",
            "Epoch: [7/7], Step: [2101/3125], Train Acc: 84.375, Val Acc: 63.2\n",
            "Epoch: [7/7], Step: [2401/3125], Train Acc: 87.5, Val Acc: 63.4\n",
            "Epoch: [7/7], Step: [2701/3125], Train Acc: 75.0, Val Acc: 62.6\n",
            "Epoch: [7/7], Step: [3001/3125], Train Acc: 81.25, Val Acc: 62.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tpRR9I63U-q1",
        "colab_type": "code",
        "outputId": "1a33aae5-d468-44ba-d4b2-9f44d1707e0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1224
        }
      },
      "cell_type": "code",
      "source": [
        "model3, train_loss_cnn3, val_acc_cnn3 = model_runner_cnn(num_epochs = 7, hidden_size = 400)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_parameters: 1413283\n",
            "Epoch: [1/7], Step: [301/3125], Train Acc: 68.75, Val Acc: 46.5\n",
            "Epoch: [1/7], Step: [601/3125], Train Acc: 50.0, Val Acc: 50.8\n",
            "Epoch: [1/7], Step: [901/3125], Train Acc: 53.125, Val Acc: 53.5\n",
            "Epoch: [1/7], Step: [1201/3125], Train Acc: 46.875, Val Acc: 54.2\n",
            "Epoch: [1/7], Step: [1501/3125], Train Acc: 56.25, Val Acc: 55.1\n",
            "Epoch: [1/7], Step: [1801/3125], Train Acc: 40.625, Val Acc: 54.9\n",
            "Epoch: [1/7], Step: [2101/3125], Train Acc: 68.75, Val Acc: 56.2\n",
            "Epoch: [1/7], Step: [2401/3125], Train Acc: 68.75, Val Acc: 56.2\n",
            "Epoch: [1/7], Step: [2701/3125], Train Acc: 56.25, Val Acc: 56.5\n",
            "Epoch: [1/7], Step: [3001/3125], Train Acc: 46.875, Val Acc: 56.8\n",
            "Epoch: [2/7], Step: [301/3125], Train Acc: 65.625, Val Acc: 57.7\n",
            "Epoch: [2/7], Step: [601/3125], Train Acc: 62.5, Val Acc: 57.7\n",
            "Epoch: [2/7], Step: [901/3125], Train Acc: 56.25, Val Acc: 59.6\n",
            "Epoch: [2/7], Step: [1201/3125], Train Acc: 78.125, Val Acc: 58.6\n",
            "Epoch: [2/7], Step: [1501/3125], Train Acc: 56.25, Val Acc: 59.8\n",
            "Epoch: [2/7], Step: [1801/3125], Train Acc: 59.375, Val Acc: 59.6\n",
            "Epoch: [2/7], Step: [2101/3125], Train Acc: 65.625, Val Acc: 59.4\n",
            "Epoch: [2/7], Step: [2401/3125], Train Acc: 62.5, Val Acc: 60.5\n",
            "Epoch: [2/7], Step: [2701/3125], Train Acc: 56.25, Val Acc: 59.4\n",
            "Epoch: [2/7], Step: [3001/3125], Train Acc: 59.375, Val Acc: 60.5\n",
            "Epoch: [3/7], Step: [301/3125], Train Acc: 71.875, Val Acc: 60.1\n",
            "Epoch: [3/7], Step: [601/3125], Train Acc: 62.5, Val Acc: 60.9\n",
            "Epoch: [3/7], Step: [901/3125], Train Acc: 62.5, Val Acc: 60.3\n",
            "Epoch: [3/7], Step: [1201/3125], Train Acc: 75.0, Val Acc: 60.3\n",
            "Epoch: [3/7], Step: [1501/3125], Train Acc: 68.75, Val Acc: 60.4\n",
            "Epoch: [3/7], Step: [1801/3125], Train Acc: 65.625, Val Acc: 59.3\n",
            "Epoch: [3/7], Step: [2101/3125], Train Acc: 68.75, Val Acc: 61.8\n",
            "Epoch: [3/7], Step: [2401/3125], Train Acc: 81.25, Val Acc: 61.5\n",
            "Epoch: [3/7], Step: [2701/3125], Train Acc: 53.125, Val Acc: 60.5\n",
            "Epoch: [3/7], Step: [3001/3125], Train Acc: 71.875, Val Acc: 63.0\n",
            "Epoch: [4/7], Step: [301/3125], Train Acc: 87.5, Val Acc: 63.6\n",
            "Epoch: [4/7], Step: [601/3125], Train Acc: 68.75, Val Acc: 62.4\n",
            "Epoch: [4/7], Step: [901/3125], Train Acc: 71.875, Val Acc: 62.5\n",
            "Epoch: [4/7], Step: [1201/3125], Train Acc: 75.0, Val Acc: 62.7\n",
            "Epoch: [4/7], Step: [1501/3125], Train Acc: 68.75, Val Acc: 61.3\n",
            "Epoch: [4/7], Step: [1801/3125], Train Acc: 53.125, Val Acc: 62.7\n",
            "Epoch: [4/7], Step: [2101/3125], Train Acc: 56.25, Val Acc: 62.0\n",
            "Epoch: [4/7], Step: [2401/3125], Train Acc: 71.875, Val Acc: 62.0\n",
            "Epoch: [4/7], Step: [2701/3125], Train Acc: 46.875, Val Acc: 62.9\n",
            "Epoch: [4/7], Step: [3001/3125], Train Acc: 65.625, Val Acc: 62.6\n",
            "Epoch: [5/7], Step: [301/3125], Train Acc: 53.125, Val Acc: 63.7\n",
            "Epoch: [5/7], Step: [601/3125], Train Acc: 75.0, Val Acc: 63.6\n",
            "Epoch: [5/7], Step: [901/3125], Train Acc: 71.875, Val Acc: 63.4\n",
            "Epoch: [5/7], Step: [1201/3125], Train Acc: 78.125, Val Acc: 65.2\n",
            "Epoch: [5/7], Step: [1501/3125], Train Acc: 68.75, Val Acc: 63.4\n",
            "Epoch: [5/7], Step: [1801/3125], Train Acc: 75.0, Val Acc: 62.5\n",
            "Epoch: [5/7], Step: [2101/3125], Train Acc: 71.875, Val Acc: 64.9\n",
            "Epoch: [5/7], Step: [2401/3125], Train Acc: 53.125, Val Acc: 64.2\n",
            "Epoch: [5/7], Step: [2701/3125], Train Acc: 84.375, Val Acc: 64.4\n",
            "Epoch: [5/7], Step: [3001/3125], Train Acc: 65.625, Val Acc: 63.1\n",
            "Epoch: [6/7], Step: [301/3125], Train Acc: 78.125, Val Acc: 62.7\n",
            "Epoch: [6/7], Step: [601/3125], Train Acc: 75.0, Val Acc: 63.7\n",
            "Epoch: [6/7], Step: [901/3125], Train Acc: 71.875, Val Acc: 63.4\n",
            "Epoch: [6/7], Step: [1201/3125], Train Acc: 65.625, Val Acc: 64.7\n",
            "Epoch: [6/7], Step: [1501/3125], Train Acc: 65.625, Val Acc: 64.6\n",
            "Epoch: [6/7], Step: [1801/3125], Train Acc: 78.125, Val Acc: 62.9\n",
            "Epoch: [6/7], Step: [2101/3125], Train Acc: 62.5, Val Acc: 64.2\n",
            "Epoch: [6/7], Step: [2401/3125], Train Acc: 71.875, Val Acc: 65.4\n",
            "Epoch: [6/7], Step: [2701/3125], Train Acc: 75.0, Val Acc: 63.5\n",
            "Epoch: [6/7], Step: [3001/3125], Train Acc: 81.25, Val Acc: 65.4\n",
            "Epoch: [7/7], Step: [301/3125], Train Acc: 59.375, Val Acc: 64.8\n",
            "Epoch: [7/7], Step: [601/3125], Train Acc: 65.625, Val Acc: 66.1\n",
            "Epoch: [7/7], Step: [901/3125], Train Acc: 46.875, Val Acc: 64.3\n",
            "Epoch: [7/7], Step: [1201/3125], Train Acc: 78.125, Val Acc: 64.0\n",
            "Epoch: [7/7], Step: [1501/3125], Train Acc: 78.125, Val Acc: 65.5\n",
            "Epoch: [7/7], Step: [1801/3125], Train Acc: 75.0, Val Acc: 65.6\n",
            "Epoch: [7/7], Step: [2101/3125], Train Acc: 62.5, Val Acc: 65.1\n",
            "Epoch: [7/7], Step: [2401/3125], Train Acc: 84.375, Val Acc: 64.6\n",
            "Epoch: [7/7], Step: [2701/3125], Train Acc: 78.125, Val Acc: 64.9\n",
            "Epoch: [7/7], Step: [3001/3125], Train Acc: 68.75, Val Acc: 63.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gFdjkLYgW2Pw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model, train_loss_cnn1, val_acc_cnn1 = model_runner_cnn(num_epochs = 10, weight_decay = 0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x-7VcSZ-jTEc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model6, train_loss_cnn6, val_acc_cnn6 = model_runner_cnn(num_epochs = 10, interact='sub')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cuyiv2j9kocY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model8, train_loss_cnn8, val_acc_cnn8 = model_runner_cnn(num_epochs = 10, interact='mult')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ov8_Bqs6kvon",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model9, train_loss_cnn9, val_acc_cnn9 = model_runner_cnn(num_epochs = 7, hidden_size = 300, interact = 'concat')\n",
        "#learning_rate=2e-4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IucXiv7FjK8u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model10, train_loss_cnn10, val_acc_cnn10 = model_runner_cnn(num_epochs = 7, hidden_size = 400, interact = 'concat')\n",
        "#learning_rate=2e-4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZI1YjlXx7pfV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# CNN, with concatenated hidden state\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, kernel_size, padding_size, len_sen, interact='concat', dropOut = True):\n",
        "        # RNN Accepts the following hyperparams:\n",
        "        # emb_size: Embedding Size\n",
        "        # hidden_size: Hidden Size of layer in RNN\n",
        "        # num_layers: number of layers in RNN\n",
        "        # num_classes: number of output classes\n",
        "        # vocab_size: vocabulary size\n",
        "        super(CNN, self).__init__()\n",
        "        #length of sentence, length of feature size\n",
        "        self.len_sen = len_sen\n",
        "        self.interact = interact\n",
        "        self.Lout_size = self.len_sen + 2*padding_size - 1*(kernel_size-1) \n",
        "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
        "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=kernel_size, padding=padding_size)\n",
        "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=kernel_size, padding=padding_size)\n",
        "        self.dropOut_ = dropOut\n",
        "        if self.dropOut_ == True:\n",
        "            self.dropOut = nn.Dropout(0.2)\n",
        "        #self.rnn_c2 = nn.RNN(emb_size, hidden_size, num_layers, batch_first=True)\n",
        "        if self.interact == 'concat':\n",
        "            self.linear_1 = nn.Linear(self.Lout_size * 2, self.Lout_size)\n",
        "        else:\n",
        "            self.linear_1 = nn.Linear(self.Lout_size, self.Lout_size)\n",
        "        self.linear_2 = nn.Linear(self.Lout_size, num_classes)\n",
        "        self.interact_method = {\n",
        "            'concat': lambda x, y: torch.cat((x, y), 1),\n",
        "            'mult': lambda x, y: x * y,\n",
        "            'sub': lambda x, y: x - y\n",
        "        }\n",
        "\n",
        "    def forward(self, c1, c2, lengths):\n",
        "        # reset hidden state\n",
        "        #print(c1.size())\n",
        "        batch_size, seq_len, emb_size = c1.size()\n",
        "\n",
        "        #print(self.hidden_1.size())\n",
        "        # get embedding of characters\n",
        "        c1_hidden = self.conv1(c1.transpose(1,2)).transpose(1,2)\n",
        "        c1_hidden = F.relu(c1_hidden.contiguous().view(-1, c1_hidden.size(-1))).view(batch_size, seq_len, c1_hidden.size(-1))\n",
        "        c1_hidden = self.conv2(c1_hidden.transpose(1,2)).transpose(1,2)\n",
        "        c1_hidden = F.relu(c1_hidden.contiguous().view(-1, c1_hidden.size(-1))).view(batch_size, seq_len, c1_hidden.size(-1))\n",
        "        \n",
        "        c2_hidden = self.conv1(c2.transpose(1,2)).transpose(1,2)\n",
        "        c2_hidden = F.relu(c2_hidden.contiguous().view(-1, c2_hidden.size(-1))).view(batch_size, seq_len, c2_hidden.size(-1))\n",
        "        c2_hidden = self.conv2(c2_hidden.transpose(1,2)).transpose(1,2)\n",
        "        c2_hidden = F.relu(c2_hidden.contiguous().view(-1, c2_hidden.size(-1))).view(batch_size, seq_len, c2_hidden.size(-1))\n",
        "        \n",
        "        #Concat, max pool\n",
        "        concated_hidden = self.interact_method[self.interact](c1_hidden, c2_hidden)\n",
        "        concated_hidden, _ = torch.max(concated_hidden, dim=2)\n",
        "        if self.dropOut_ == True:\n",
        "            concated_hidden = self.dropOut(concated_hidden)\n",
        "        #print('embed_size:',embed.size())\n",
        "        # pack padded sequence\n",
        "        \n",
        "        # fprop though RNN\n",
        "        \n",
        "        # print('rnn_out_size:',rnn_out.size())\n",
        "        # print('hidden_size:',self.hidden.size())\n",
        "        # undo packing\n",
        "        \n",
        "        l1_out = self.linear_1(concated_hidden)\n",
        "        l2_out = self.linear_2(l1_out)\n",
        "        #print(l2_out.size())\n",
        "        #print('rnn_out_size_2:',rnn_out.size())\n",
        "        # sum hidden activations of RNN across time\n",
        "        #rnn_out = torch.sum(rnn_out, dim=1)\n",
        "        #print('rnn_out_size_final:',rnn_out.size())\n",
        "        #logits = self.linear(rnn_out)\n",
        "        #print('logits_size_final:',logits.size())\n",
        "        #print(l2_out.size())\n",
        "        return l2_out.cuda()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bvZlHF8_Mc2n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_model(loader, model):\n",
        "    \"\"\"\n",
        "    Help function that tests the model's performance on a dataset\n",
        "    @param: loader - data loader for the dataset to test against\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    for i, (cen_1, cen_2, lengths, labels) in enumerate(loader):\n",
        "        cen_1_batch, cen_2_batch, lengths_batch, label_batch = cen_1.float().cuda(), cen_2.float().cuda(), lengths.cuda(), labels.cuda()\n",
        "        outputs = F.softmax(model(cen_1_batch, cen_2_batch, lengths_batch), dim=1).cuda()\n",
        "        predicted = outputs.max(1, keepdim=True)[1]\n",
        "        predicted = predicted.cuda()\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels.view_as(predicted).cuda()).sum().item()\n",
        "    return (100 * correct / total)\n",
        "\n",
        "def model_runner_cnn(emb_size=300, hidden_size=200, num_layers=1, num_classes=3, kernel_size=5, padding_size=2, \\\n",
        "                     interact='concat', weight_decay = 0, learning_rate=2e-4, num_epochs=5, dropOut=True):\n",
        "    if 'model' in dir():\n",
        "        del(model)\n",
        "        \n",
        "    model = CNN(emb_size=emb_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes, \\\n",
        "                kernel_size=kernel_size, padding_size=padding_size, len_sen=78, dropOut=dropOut)\n",
        "\n",
        "    learning_rate = learning_rate\n",
        "    num_epochs = num_epochs # number epoch to train\n",
        "\n",
        "    # Criterion and Optimizer\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay = weight_decay)\n",
        "    model.cuda()\n",
        "    criterion.cuda()\n",
        "\n",
        "    print('num_parameters:', sum(p.numel() for p in model.parameters()))\n",
        "    \n",
        "    # Train the model\n",
        "    total_step = len(train_loader)\n",
        "\n",
        "    train_loss_list = []\n",
        "    val_acc_list = []\n",
        "    toTerminate = False\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        if toTerminate == True:\n",
        "            break\n",
        "        for i, (cen_1, cen_2, lengths, labels) in enumerate(train_loader):\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            outputs = model(cen_1.float().cuda(), cen_2.float().cuda(), lengths.cuda())\n",
        "            #print(outputs.size())\n",
        "            #print(labels.size())\n",
        "            loss = criterion(outputs, labels.cuda())\n",
        "\n",
        "            # Backward and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if i > 0 and i % 300 == 0:\n",
        "                # validate\n",
        "                val_acc = test_model(val_loader, model)\n",
        "                val_acc_list.append(val_acc)\n",
        "                train_loss = loss.item()\n",
        "                train_loss_list.append(train_loss)\n",
        "                print('Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}, Validation Acc: {}'.format(\n",
        "                           epoch+1, num_epochs, i+1, len(train_loader), train_loss, val_acc))\n",
        "                if val_acc >= 66:\n",
        "                    toTerminate = True\n",
        "                    break\n",
        "    pkl_dumper(train_loss_list, resPath + '/train_loss_list_' + str(hidden_size) + '_' + str(interact) + str(learning_rate) + str(weight_decay) + '.p')\n",
        "    pkl_dumper(val_acc_list, resPath + '/val_acc_list_' + str(hidden_size) + '_' + str(interact) + str(learning_rate) + str(weight_decay) + '.p')\n",
        "    torch.save(model.state_dict(), resPath + '/model_' + str(hidden_size) + '_' + str(interact) + str(learning_rate) + str(weight_decay) + '.model')\n",
        "    return(model, train_loss_list, val_acc_list)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EhD_5LKlMh-r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model10, train_loss_cnn10, val_acc_cnn10 = model_runner_cnn(num_epochs = 8, hidden_size = 300, interact = 'concat', dropOut=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hUd4H0huM83J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}