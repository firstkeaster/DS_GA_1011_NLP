{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy\n",
    "import string\n",
    "import pickle as pkl\n",
    "import random\n",
    "import io\n",
    "import os\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "#import torch\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(152)\n",
    "\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nowpath = os.path.abspath('.')\n",
    "datapath = nowpath + '/hw2_data'\n",
    "picklePath = nowpath + '/hw2_data_p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train = []\n",
    "all_val = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_M = []\n",
    "all_val_M = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snli_reader(dataset, filepath):\n",
    "    with open(filepath) as f:\n",
    "        f.readline()\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            dataset.append([x.strip('. \\n') for x in line.split('\\t')])\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_reader(all_train, datapath + '/snli_train.tsv')\n",
    "snli_reader(all_val, datapath + '/snli_val.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_reader(all_train_M, datapath + '/mnli_train.tsv')\n",
    "snli_reader(all_val_M, datapath + '/mnli_val.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = string.punctuation\n",
    "def tokenize(sentence):\n",
    "    tokens = [word.strip(punctuations) for word in sentence.split(' ')]\n",
    "    return [token.lower() for token in tokens if (token not in punctuations)]\n",
    "'''def lower_case_remove_punc(parsed):\n",
    "    return [token.text.lower() for token in parsed if (token.text not in punctuations)]'''\n",
    "\n",
    "def tokenize_dataset_ngram(dataset, n):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset\n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "\n",
    "    for sample in dataset:\n",
    "        tokens = [token for token in tokenize(sample)]\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens\n",
    "\n",
    "def label_encoder(dataset, cat_rec = None):\n",
    "    if not cat_rec:\n",
    "        cat_rec = {x:i for i,x in enumerate(list(set(dataset)), 0)}\n",
    "    receiver = []\n",
    "    for data in dataset:\n",
    "        receiver.append(cat_rec[data])\n",
    "    return(receiver, cat_rec)\n",
    "\n",
    "def pkl_dumper(objct, file_name):\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pkl.dump(objct, f, protocol=None)\n",
    "    return\n",
    "\n",
    "def pkl_loader(file_name):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        objct = pkl.load(f)\n",
    "    return(objct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_dict = {}\n",
    "all_val_dict = {}\n",
    "train_tknzd_dict = {}\n",
    "val_tknzd_dict = {}\n",
    "all_train_tks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A woman is smiling while the man next to her is focused on a blue object with a pattern on it',\n",
       " 'Two people are next to each other',\n",
       " 'entailment']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Four people sit on a subway two read books , one looks at a cellphone and is wearing knee high boots',\n",
       " 'Multiple people are on a subway together , with each of them doing their own thing',\n",
       " 'entailment']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_val[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize sentences, 1gram\n",
    "all_train_dict['org_sen'], all_train_dict['alt_sen'], all_train_dict['label'] = zip(*all_train)\n",
    "all_val_dict['org_sen'], all_val_dict['alt_sen'], all_val_dict['label'] = zip(*all_val)\n",
    "train_tknzd_dict['org'], all_train_tks = tokenize_dataset_ngram(all_train_dict['org_sen'], 0)\n",
    "val_tknzd_dict['org'], _ = tokenize_dataset_ngram(all_val_dict['org_sen'], 0)\n",
    "train_tknzd_dict['alt'], _ = tokenize_dataset_ngram(all_train_dict['alt_sen'], 0)\n",
    "val_tknzd_dict['alt'], _ = tokenize_dataset_ngram(all_val_dict['alt_sen'], 0)\n",
    "# get encoded label\n",
    "encoded_label_train, cat_rec = label_encoder(all_train_dict['label'])\n",
    "encoded_label_val, _ = label_encoder(all_val_dict['label'], cat_rec)\n",
    "train_tknzd_dict['label'] = encoded_label_train\n",
    "val_tknzd_dict['label'] = encoded_label_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_dict_M = {}\n",
    "all_val_dict_M = {}\n",
    "train_tknzd_dict_M = {}\n",
    "val_tknzd_dict_M = {}\n",
    "all_train_tks_M = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize sentences, 1gram, get label\n",
    "cat_rec = {'entailment':1, 'neutral':0, 'contradiction':2}\n",
    "all_train_dict_M['org_sen'], all_train_dict_M['alt_sen'], all_train_dict_M['label'], all_train_dict_M['genre'] = zip(*all_train_M)\n",
    "all_val_dict_M['org_sen'], all_val_dict_M['alt_sen'], all_val_dict_M['label'], all_val_dict_M['genre'] = zip(*all_val_M)\n",
    "train_tknzd_dict_M['org'], all_train_tks_M = tokenize_dataset_ngram(all_train_dict_M['org_sen'], 0)\n",
    "val_tknzd_dict_M['org'], _ = tokenize_dataset_ngram(all_val_dict_M['org_sen'], 0)\n",
    "train_tknzd_dict_M['alt'], _ = tokenize_dataset_ngram(all_train_dict_M['alt_sen'], 0)\n",
    "val_tknzd_dict_M['alt'], _ = tokenize_dataset_ngram(all_val_dict_M['alt_sen'], 0)\n",
    "encoded_label_train_M, _ = label_encoder(all_train_dict_M['label'], cat_rec)\n",
    "encoded_label_val_M, _ = label_encoder(all_val_dict_M['label'], cat_rec)\n",
    "train_tknzd_dict_M['label'] = encoded_label_train_M\n",
    "val_tknzd_dict_M['label'] = encoded_label_val_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tknzd_dict_M['genre'] = all_train_dict_M['genre']\n",
    "val_tknzd_dict_M['genre'] = all_val_dict_M['genre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'telephone': 4270,\n",
       "         'fiction': 3836,\n",
       "         'slate': 4026,\n",
       "         'government': 3883,\n",
       "         'travel': 3985})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(all_train_dict_M['genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_train_dict_M['org_sen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenList = sorted([(len(x),i) for i, x in enumerate(all_train_dict_M['org_sen'])], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"um i do n't know whether state would statehood would improve their economy i do n't i do n't know that the very the act of being a state would would have any impact on on them uh i guess they would have the ability to do some taxing that they do n't have now but of course if their economy is weak that there is not much of a base on which to tax i do n't know if they suffer in a sense of income loss as being since they are n't a state whether there are monies that escape them so to speak because they are n't able to tax like a typical state would be how do you feel though about well i guess it 's to their advantage to be a territory but uh i wonder how having having been in a territory but only as a young student and my parents were in the military at the time so they did n't have ready negative feelings about being in the states in the Alaska at the time since they they voted absentee i i would imagine that it must be a little bit of a second feeling of second class citizenry uh to be in a territory that large and not being able to vote the District of Columbia people for instance are quite frustrated i think at times in their not having a Senate representative\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_dict_M['org_sen'][9866]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(254, 19642), (254, 18607), (254, 17307), (254, 16927), (254, 13061)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenList[1000:1005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['four',\n",
       " 'people',\n",
       " 'sit',\n",
       " 'on',\n",
       " 'a',\n",
       " 'subway',\n",
       " 'two',\n",
       " 'read',\n",
       " 'books',\n",
       " 'one',\n",
       " 'looks',\n",
       " 'at',\n",
       " 'a',\n",
       " 'cellphone',\n",
       " 'and',\n",
       " 'is',\n",
       " 'wearing',\n",
       " 'knee',\n",
       " 'high',\n",
       " 'boots']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_tknzd_dict['org'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors_py3(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for i, line in enumerate(fin):\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = list(map(float, tokens[1:]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load pre-trained embeddings\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for i, line in enumerate(fin):\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2emb = load_vectors_py3(datapath + '/wiki-news-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2emb['<unk>'] = [1] * 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2emb['<pad>'] = [0] * 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max len getter\n",
    "max_cen_len_org = max([len(cen) for cen in train_tknzd_dict_M['org'] + val_tknzd_dict_M['org']])\n",
    "max_cen_len_alt = max([len(cen) for cen in train_tknzd_dict_M['alt'] + val_tknzd_dict_M['alt']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14467 48\n"
     ]
    }
   ],
   "source": [
    "print(max_cen_len_org, max_cen_len_alt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding the tknzd sentences\n",
    "def tknzd_padder(dataset, max_len, padder = '<pad>'):\n",
    "    padded = []\n",
    "    for data in dataset:\n",
    "        if len(data) > max_len:\n",
    "            data[:] = data[:max_len]\n",
    "        padded.append(data + (max_len-len(data)) * [padder])\n",
    "    return(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedded_dict = {}\n",
    "val_embedded_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedded_dict['label'] = train_tknzd_dict['label']\n",
    "val_embedded_dict['label'] = val_tknzd_dict['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tk_to_embed(dataset, tk2emb):\n",
    "    list_embedded = []\n",
    "    for data in dataset:\n",
    "        list_embedded.append([tk2emb.get(char, tk2emb['<unk>']) for char in data])\n",
    "    return(list_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tknzd_padded = {}\n",
    "val_tknzd_padded = {}\n",
    "train_tknzd_padded['org'] = tknzd_padder(train_tknzd_dict['org'], max_cen_len_org)\n",
    "train_tknzd_padded['alt'] = tknzd_padder(train_tknzd_dict['alt'], max_cen_len_org)\n",
    "val_tknzd_padded['org'] = tknzd_padder(val_tknzd_dict['org'], max_cen_len_org)\n",
    "val_tknzd_padded['alt'] = tknzd_padder(val_tknzd_dict['alt'], max_cen_len_org)\n",
    "train_embedded_dict['org'] = tk_to_embed(train_tknzd_padded['org'], token2emb)\n",
    "train_embedded_dict['alt'] = tk_to_embed(train_tknzd_padded['alt'], token2emb)\n",
    "val_embedded_dict['org'] = tk_to_embed(val_tknzd_padded['org'], token2emb)\n",
    "val_embedded_dict['alt'] = tk_to_embed(val_tknzd_padded['alt'], token2emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedded_dict_M = {}\n",
    "val_embedded_dict_M = {}\n",
    "train_embedded_dict_M['label'] = train_tknzd_dict_M['label']\n",
    "val_embedded_dict_M['label'] = val_tknzd_dict_M['label']\n",
    "train_embedded_dict_M['genre'] = train_tknzd_dict_M['genre']\n",
    "val_embedded_dict_M['genre'] = val_tknzd_dict_M['genre']\n",
    "train_tknzd_padded_M = {}\n",
    "val_tknzd_padded_M = {}\n",
    "train_tknzd_padded_M['org'] = tknzd_padder(train_tknzd_dict_M['org'], 78)\n",
    "train_tknzd_padded_M['alt'] = tknzd_padder(train_tknzd_dict_M['alt'], 78)\n",
    "val_tknzd_padded_M['org'] = tknzd_padder(val_tknzd_dict_M['org'], 78)\n",
    "val_tknzd_padded_M['alt'] = tknzd_padder(val_tknzd_dict_M['alt'], 78)\n",
    "train_embedded_dict_M['org'] = tk_to_embed(train_tknzd_padded_M['org'], token2emb)\n",
    "train_embedded_dict_M['alt'] = tk_to_embed(train_tknzd_padded_M['alt'], token2emb)\n",
    "val_embedded_dict_M['org'] = tk_to_embed(val_tknzd_padded_M['org'], token2emb)\n",
    "val_embedded_dict_M['alt'] = tk_to_embed(val_tknzd_padded_M['alt'], token2emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 78\n",
      "78 78\n"
     ]
    }
   ],
   "source": [
    "print(max([len(cen) for cen in train_tknzd_padded_M['org']]), min([len(cen) for cen in train_tknzd_padded_M['org']]))\n",
    "print(max([len(cen) for cen in train_tknzd_padded_M['alt']]), min([len(cen) for cen in train_tknzd_padded_M['alt']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'telephone'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_embedded_dict_M['genre'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fiction'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embedded_dict_M['genre'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_dumper(train_embedded_dict_M,picklePath + '/train_embedded_dict_pad_samelen_M_try2.p')\n",
    "pkl_dumper(val_embedded_dict_M,picklePath + '/val_embedded_dict_pad_samelen_M_try2.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 783M\r\n",
      "-rw-r----- 1 fs1520 fs1520  64M Oct 18 13:05 train_embedded_dict.p\r\n",
      "-rw-r----- 1 fs1520 fs1520 110M Oct 18 16:13 train_embedded_dict_pad.p\r\n",
      "-rw-r----- 1 fs1520 fs1520  83M Oct 19 20:57 train_embedded_dict_pad_samelen_1019.p\r\n",
      "-rw-r----- 1 fs1520 fs1520 129M Oct 18 17:50 train_embedded_dict_pad_samelen.p\r\n",
      "-rw-r----- 1 fs1520 fs1520  83M Oct 19 20:57 train_embedded_dict_pad_samelen_try.p\r\n",
      "-rw-r----- 1 fs1520 fs1520  64M Oct 18 13:05 val_embedded_dict.p\r\n",
      "-rw-r----- 1 fs1520 fs1520 110M Oct 18 16:14 val_embedded_dict_pad.p\r\n",
      "-rw-r----- 1 fs1520 fs1520 6.9M Oct 19 20:57 val_embedded_dict_pad_samelen_1019.p\r\n",
      "-rw-r----- 1 fs1520 fs1520 129M Oct 18 17:51 val_embedded_dict_pad_samelen.p\r\n",
      "-rw-r----- 1 fs1520 fs1520 6.9M Oct 19 20:57 val_embedded_dict_pad_samelen_try.p\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh hw2_data_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_tknzd_padded_samelen = {}\n",
    "val_tknzd_padded_samelen = {}\n",
    "train_tknzd_padded_samelen['org'] = tknzd_padder(train_tknzd_dict['org'], max_cen_len_org)\n",
    "train_tknzd_padded_samelen['alt'] = tknzd_padder(train_tknzd_dict['alt'], max_cen_len_org)\n",
    "val_tknzd_padded_samelen['org'] = tknzd_padder(val_tknzd_dict['org'], max_cen_len_org)\n",
    "val_tknzd_padded_samelen['alt'] = tknzd_padder(val_tknzd_dict['alt'], max_cen_len_org)\n",
    "train_tknzd_padded_samelen['label'] = [lab-1 for lab in train_tknzd_dict['label']]\n",
    "val_tknzd_padded_samelen['label'] = [lab-1 for lab in val_tknzd_dict['label']]\n",
    "train_tknzd_padded_samelen['org'] = tk_to_embed(train_tknzd_padded_samelen['org'], token2emb)\n",
    "train_tknzd_padded_samelen['alt'] = tk_to_embed(train_tknzd_padded_samelen['alt'], token2emb)\n",
    "val_tknzd_padded_samelen['org'] = tk_to_embed(val_tknzd_padded_samelen['org'], token2emb)\n",
    "val_tknzd_padded_samelen['alt'] = tk_to_embed(val_tknzd_padded_samelen['alt'], token2emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_dumper(train_tknzd_padded_samelen,picklePath + '/train_embedded_dict_pad_samelen.p')\n",
    "pkl_dumper(val_tknzd_padded_samelen,picklePath + '/val_embedded_dict_pad_samelen.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedded_dict_why = pkl_loader(picklePath + '/train_embedded_dict_pad_samelen.p')\n",
    "val_embedded_dict_why = pkl_loader(picklePath + '/val_embedded_dict_pad_samelen.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_embedded_dict['alt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 78\n",
      "78 78\n"
     ]
    }
   ],
   "source": [
    "print(max([len(cen) for cen in train_embedded_dict['org']]), min([len(cen) for cen in train_embedded_dict['org']]))\n",
    "print(max([len(cen) for cen in train_embedded_dict['alt']]), min([len(cen) for cen in train_embedded_dict['alt']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 3, 3]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embedded_dict['label'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
